<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Andy Dote</title><link>https://andydote.co.uk/post/</link><description>Recent content in Posts on Andy Dote</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 19 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://andydote.co.uk/post/rss.xml" rel="self" type="application/rss+xml"/><item><title>Content based change detection with Make</title><link>https://andydote.co.uk/2022/09/19/make-content-hash/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2022/09/19/make-content-hash/</guid><description>On several occasions when building complex projects, I have been tempted to set up Bazel to help speed up the build process; after all, it has a lot to offer: only building what has changed, caching built artifacts, and sharing that cache between machines for even more speed.
TLDR We can use Make and a couple of short shell scripts to implement file content-based caching and read/write that cache to remote storage, such as S3.</description></item><item><title>Embedding ain't easy, but its alright</title><link>https://andydote.co.uk/2022/09/14/embed/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2022/09/14/embed/</guid><description>I write a lot of blog posts which contain code snippets. Like most people, I have done this using a fenced codeblock in markdown which is fine for short blocks of code.
The problem occours when I am embedding code from another repository; often there will be tweaks or bug fixes, and keeping the code in the blog post in sync with the code in the other repo is annoying and manual.</description></item><item><title>The reports of UML's death are greatly exaggerated</title><link>https://andydote.co.uk/2022/09/11/uml-isnt-dead/</link><pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2022/09/11/uml-isnt-dead/</guid><description>This is in response to the recent posts about the death of UML; while I think some parts of UML have fallen ill, the remaining parts are still alive, and useful to this day.
TLDR Out of 14 types of diagram there are 3 that I use on a regular basis: Activity Diagram, State Machine Diagram, and Sequence Diagram. I think the Timing Diagram is borderline, but I can only think of a couple of occasions when it has been useful.</description></item><item><title>Pulumi Conditional Infrastructure for Speed</title><link>https://andydote.co.uk/2022/07/17/pulumi-faster-processes/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2022/07/17/pulumi-faster-processes/</guid><description>One of the reasons I prefer Pulumi over Terraform is the additional control I have over my processes due to the fact that it&amp;rsquo;s a programming language.
For example, I have a CLI, that creates a cluster of machines for a user; the machines use IAM Authentication with Vault so that they can request certificates on boot. The trouble with this application is that it is slow; it takes 175 seconds on average to provision the machines, write the IAM information to Vault, and then re-run the cloud-init script on all the machines in the cluster (as when they first booted, the configuration hadn&amp;rsquo;t been written to Vault yet.</description></item><item><title>An NGINX and DNS based outage</title><link>https://andydote.co.uk/2022/04/23/nginx-dns/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2022/04/23/nginx-dns/</guid><description>I recently encountered a behaviour in Nginx that I didn&amp;rsquo;t expect and caused a production outage in the process. While I would love to blame DNS for this, as it&amp;rsquo;s usually the cause of most network-related issues, in this case, the fault lies with Nginx.
I was running a very simple Nginx proxy, relaying an internal service to the outside world. The internal service is behind an AWS ALB, and the Nginx configuration was proxying to the ALB&amp;rsquo;s FQDN:</description></item><item><title>The Operator Pattern in Nomad</title><link>https://andydote.co.uk/2021/11/22/nomad-operator-pattern/</link><pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/11/22/nomad-operator-pattern/</guid><description>The Operator Pattern from Kubernetes is an excellent way of handling tasks in a cluster in an automated way, for example, provisioning applications, running backups, requesting certificates, and injecting chaos testing.
As a Nomad user, I wanted to do something similar for my clusters, so I set about seeing how it would be possible. It turns out; it is much easier than I expected! While Nomad doesn&amp;rsquo;t support the idea of Custom Resource Definitions, we can achieve an operator by utilising a regular Nomad job and the nomad HTTP API.</description></item><item><title>How do you tag docker images?</title><link>https://andydote.co.uk/2021/11/10/docker-tagging/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/11/10/docker-tagging/</guid><description>An interesting question came up at work today: how do you tag your Docker images? In previous projects, I&amp;rsquo;ve always used a short git sha, or sometimes a semver, but with no great consistency.
As luck would have it, I had pushed for a change in tagging format at a client not so long ago as the method we were using didn&amp;rsquo;t make a lot of sense and, worst of all, it was a manual process.</description></item><item><title>The Problem with CPUs and Kubernetes</title><link>https://andydote.co.uk/2021/06/02/os-cpus-and-kubernetes/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/06/02/os-cpus-and-kubernetes/</guid><description>Key Takeaway: os .cpus() returns the number of cores on a Kubernetes host, not the number of cores assigned to a pod.
Investigating excessive memory usage Recently, when I was looking through a cluster health dashboard for a Kubernetes cluster, I noticed that one of the applications deployed was using a considerable amount of RAM - way more than I thought could be reasonable. Each instance (pod) of the application used approximately 8 GB of RAM, which was definitely excessive for a reasonably simple NodeJS webserver.</description></item><item><title>Adding Observability to Vault</title><link>https://andydote.co.uk/2021/05/27/vault-observe/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/05/27/vault-observe/</guid><description>One of the things I like to do when setting up a Vault cluster is to visualise all the operations Vault is performing, which helps see usage patterns changing, whether there are lots of failed requests coming in, and what endpoints are receiving the most traffic.
While Vault has a lot of data available in Prometheus telemetry, the kind of information I am after is best taken from the Audit backend.</description></item><item><title>Getting NodeJS OpenTelemetry data into NewRelic</title><link>https://andydote.co.uk/2021/03/12/nodejs-opentelemetry-newrelic/</link><pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/03/12/nodejs-opentelemetry-newrelic/</guid><description>I had the need to get some OpenTelemetry data out of a NodeJS application, and into NewRelic&amp;rsquo;s distributed tracing service, but found that there is no way to do it directly, and in this use case, adding a separate collector is more hassle than it&amp;rsquo;s worth.
Luckily, there is an NodeJS OpenTelemetry library which can report to Zipkin, and NewRelic can also ingest Zipkin format data.
To use it was relatively straight forward:</description></item><item><title>Observability with Infrastructure as Code</title><link>https://andydote.co.uk/2021/03/01/observability-with-infrastructure-as-code/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/03/01/observability-with-infrastructure-as-code/</guid><description>This article was originally published on the Pulumi blog.
When using the Pulumi Automation API to create applications which can provision infrastructure, it is very handy to be able to use observability techniques to ensure the application functions correctly and to help see where performance bottlenecks are.
One of the applications I work on creates a VPC and Bastion host and then stores the credentials into a Vault instance. The problem is that the “create infrastructure” part is an opaque blob, in that I can see it takes 129 seconds to create, but I can’t see what it’s doing, or why it takes this amount of time.</description></item><item><title>Forking Multi Container Docker Builds</title><link>https://andydote.co.uk/2020/11/03/docker-multi-output/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/11/03/docker-multi-output/</guid><description>Following on from my last post on Isolated Multistage Docker Builds, I thought it would be useful to cover another advantage to splitting your dockerfiles: building different output containers from a common base.
The Problem When I have an application which when built, needs to have all assets in one container, and a subset of assets in a second container.
For example, writing a node webapp, where you want the compiled/bundled static assets available in the container as a fallback, and also stored in an nginx container for serving.</description></item><item><title>Isolated Docker Multistage Images</title><link>https://andydote.co.uk/2020/11/01/docker-multistage-containers/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/11/01/docker-multistage-containers/</guid><description>Often when building applications, I will use a multistage docker build for output container size and efficiency, but will run the build in two halves, to make use of the extra assets in the builder container, something like this:
docker build \ --target builder \ -t builder:$GIT_COMMIT \ . docker run --rm \ -v &amp;#34;$PWD/artefacts/tests:/artefacts/tests&amp;#34; \ builder:$GIT_COMMIT \ yarn ci:test docker run --rm \ -v &amp;#34;$PWD/artefacts/lint:/artefacts/lint&amp;#34; \ builder:$GIT_COMMIT \ yarn ci:lint docker build \ --cache-from builder:$GIT_COMMIT \ --target output \ -t app:$GIT_COMMIT \ .</description></item><item><title>Better BASHing Through Technology</title><link>https://andydote.co.uk/2020/08/28/better-bashing-through-technology/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/08/28/better-bashing-through-technology/</guid><description>I write a lot of bash scripts for both my day job and my personal projects, and while they are functional, bash scripts always seem to lack that structure that I want, especially when compared to writing something in Go or C#. The main problem I have with bash scripts is that when I use functions, I lose the ability to log things.
For example the get_config_path function will print the path to the configuration file, which will get consumed by the do_work function:</description></item><item><title>Sharing Docker Layers Between Build Agents</title><link>https://andydote.co.uk/2020/05/14/docker-layer-sharing/</link><pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/05/14/docker-layer-sharing/</guid><description>Recently, I noticed that when we pull a new version of our application&amp;rsquo;s docker container, it fetches all layers, not just the ones that change.
The problem is that we use ephemeral build agents, which means that each version of the application is built using a different agent, so Docker doesn&amp;rsquo;t know how to share the layers used. While we can pull the published container before we run the build, this only helps with the final stage of the build.</description></item><item><title>Service Mesh with Consul Connect (and Nomad)</title><link>https://andydote.co.uk/2020/05/04/service-mesh-consul-connect/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/05/04/service-mesh-consul-connect/</guid><description>When it comes to implementing a new feature in an application&amp;rsquo;s ecosystem, I don&amp;rsquo;t like spending my innovation tokens unless I have to, so I try not to add new tools to my infrastructure unless I really need them.
This same approach comes when I either want, need, or have been told, to implement a Service Mesh. This means I don&amp;rsquo;t instantly setup Istio. Not because it&amp;rsquo;s bad - far from it - but because it&amp;rsquo;s extra complexity I would rather avoid, unless I need it.</description></item><item><title>Observability Without Honeycomb</title><link>https://andydote.co.uk/2020/03/15/observability-without-honeycomb/</link><pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/03/15/observability-without-honeycomb/</guid><description>Before I start on this, I want to make it clear that if you can buy Honeycomb, you should. Outlined below is how I started to add observability to an existing codebase which already had the ELK stack available, and was unable to use Honeycomb. My hope, in this case, is that I can demonstrate how much value observability gives, and also show how much more value you would get with an excellent tool, such as Honeycomb.</description></item><item><title>Nomad Isolated Exec</title><link>https://andydote.co.uk/2020/02/29/nomad-isolated-exec/</link><pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/02/29/nomad-isolated-exec/</guid><description>One of the many features of Nomad that I like is the ability to run things other than Docker containers. It has built-in support for Java, QEMU, and Rkt, although the latter is deprecated. Besides these inbuilt &amp;ldquo;Task Drivers&amp;rdquo; there are community maintained ones too, covering Podman, LXC, Firecraker and BSD Jails, amongst others.
The one I want to talk about today, however, is called exec. This Task Driver runs any given executable, so if you have an application which you don&amp;rsquo;t want (or can&amp;rsquo;t) put into a container, you can still schedule it with Nomad.</description></item><item><title>Consul DNS Fowarding in Alpine, revisited</title><link>https://andydote.co.uk/2019/12/30/consul-alpine-dns-revisited/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/12/30/consul-alpine-dns-revisited/</guid><description>I noticed when running an Alpine based virtual machine with Consul DNS forwarding set up, that sometimes the machine couldn&amp;rsquo;t resolve *.consul domains, but not in a consistent manner. Inspecting the logs looked like the request was being made and responded to successfully, but the result was being ignored.
After a lot of googling and frustration, I was able to track down that it&amp;rsquo;s down to a difference (or optimisation) in musl libc, which glibc doesn&amp;rsquo;t do.</description></item><item><title>Libvirt Hostname Resolution</title><link>https://andydote.co.uk/2019/12/22/libvirt-hostname-resolution/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/12/22/libvirt-hostname-resolution/</guid><description>I use Vagrant when testing new machines and experimenting locally with clusters, and since moving (mostly) to Linux, I have been using the LibVirt Plugin to create the virtual machines. Not only is it significantly faster than Hyper-V was on windows, but it also means I don&amp;rsquo;t need to use Oracle products, so it&amp;rsquo;s win-win really.
The only configuration challenge I have had with it is setting up VM hostname resolution, and as I forget how to do it each time, I figured I should write about it.</description></item><item><title>Nomad Good, Kubernetes Bad</title><link>https://andydote.co.uk/2019/11/21/nomad-good-kubernetes-bad/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/11/21/nomad-good-kubernetes-bad/</guid><description>I will update this post as I learn more (both positive and negative), and is here to be linked to when people ask me why I don&amp;rsquo;t like Kubernetes, and why I would pick Nomad in most situations if I chose to use an orchestrator at all.
TLDR: I don&amp;rsquo;t like complexity, and Kubernetes has more complexity than benefits.
Operational Complexity Operating Nomad is very straight forward. There are very few moving parts, so the number of things which can go wrong is significantly reduced.</description></item><item><title>Creating a Vault instance with a TLS Consul Cluster</title><link>https://andydote.co.uk/2019/10/06/vault-consul-bootstrap/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/10/06/vault-consul-bootstrap/</guid><description>So we want to set up a Vault instance, and have it&amp;rsquo;s storage be a TLS based Consul cluster. The problem is that the Consul cluster needs Vault to create the certificates for TLS, which is quite the catch-22. Luckily for us, quite easy to solve:
Start a temporary Vault instance as an intermediate ca Launch Consul cluster, using Vault to generate certificates Destroy temporary Vault instance Start a permanent Vault instance, with Consul as the store Reprovision the Consul cluster with certificates from the new Vault instance There is a repository on Github with all the scripts used, and a few more details on some options.</description></item><item><title>Consul DNS Fowarding in Ubuntu, revisited</title><link>https://andydote.co.uk/2019/09/24/consul-ubuntu-dns-revisited/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/09/24/consul-ubuntu-dns-revisited/</guid><description>I was recently using my Hashibox for a test, and I noticed the DNS resolution didn&amp;rsquo;t seem to work. This was a bit worrying, as I have written about how to do DNS resolution with Consul forwarding in Ubuntu, and apparently something is wrong with how I do it. Interestingly, the Alpine version works fine, so it appears there is something not quite working with how I am configuring Systemd-resolved.</description></item><item><title>Creating a TLS enabled Consul cluster</title><link>https://andydote.co.uk/2019/09/14/consul-tls-cluster/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/09/14/consul-tls-cluster/</guid><description>This post is going to go through how to set up a Consul cluster to communicate over TLS. I will be using Vagrant to create three machines locally, which will form my cluster, and in the provisioning step will use Vault to generate the certificates needed.
How to securely communicate with Vault to get the TLS certificates is out of scope for this post.
Host Configuration Unless you already have Vault running somewhere on your network, or have another mechanism to generate TLS certificates for each machine, you&amp;rsquo;ll need to start and configure Vault on the Host machine.</description></item><item><title>Using Vault as a Development CA</title><link>https://andydote.co.uk/2019/08/25/vault-development-ca/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/08/25/vault-development-ca/</guid><description>Often when developing or testing some code, I need (or want) to use SSL, and one of the easiest ways to do that is to use Vault. However, it gets pretty annoying having to generate a new CA for each project, and import the CA cert into windows (less painful in Linux, but still annoying), especially as I forget which cert is in use, and accidentally clean up the wrong ones.</description></item><item><title>Architecture Decision Records</title><link>https://andydote.co.uk/2019/06/29/architecture-decision-records/</link><pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/29/architecture-decision-records/</guid><description>This is a text version of a short talk (affectionately known as a &amp;ldquo;Coffee Bag&amp;rdquo;) I gave at work this week, on Architecture Design Records. You can see the slides here, but there isn&amp;rsquo;t a recording available, unfortunately.
It should be noted; these are not to replace full architecture diagrams; you should definitely still write C4 Models to cover the overall architecture. ADRs are for the details, such as serializer formats, convention-over-configuration details, number precisions for timings, or which metrics library is used and why.</description></item><item><title>Canary Routing with Traefik in Nomad</title><link>https://andydote.co.uk/2019/06/23/nomad-traefik-canary/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/23/nomad-traefik-canary/</guid><description>I wanted to implement canary routing for some HTTP services deployed via Nomad the other day, but rather than having the traffic split by weighting to the containers, I wanted to direct the traffic based on a header.
My first choice of tech was to use Fabio, but it only supports routing by URL prefix, and additionally with a route weight. While I was at JustDevOps in Poland, I heard about another router/loadbalancer which worked in a similar way to Fabio: Traefik.</description></item><item><title>Feature Toggles: Reducing Coupling</title><link>https://andydote.co.uk/2019/06/11/feature-toggles-reducing-coupling/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/11/feature-toggles-reducing-coupling/</guid><description>One of the points I make in my Feature Toggles talk is that you shouldn&amp;rsquo;t be querying a toggle&amp;rsquo;s status all over your codebase. Ideally, each toggle gets checked in as few places as possible - preferably only one place. The advantage of doing this is that very little of your codebase needs to be coupled to the toggles (either the toggle itself or the library/system for managing toggles itself).</description></item><item><title>Feature Toggles: Branch by Abstraction</title><link>https://andydote.co.uk/2019/06/03/feature-toggles-branch-by-abstraction/</link><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/03/feature-toggles-branch-by-abstraction/</guid><description>Recently, I was asked if I could provide an example of Branch By Abstraction when dealing with feature toggles. As this has come up a few times, I thought a blog post would be a good idea so I can refer others to it later too.
The Context As usual, this is some kind of backend (micro)service, and it will send email messages somehow. We will start with two implementations of message sending: the &amp;ldquo;current&amp;rdquo; version; which is synchronous, and a &amp;ldquo;new&amp;rdquo; version; which is async.</description></item><item><title>Configuring Consul DNS Forwarding in Alpine Linux</title><link>https://andydote.co.uk/2019/05/31/consul-dns-forwarding-alpine/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/05/31/consul-dns-forwarding-alpine/</guid><description>DEPRECATED - This has a race condition! Please see this post for an updated version which works!
Following on from the post the other day on setting up DNS forwarding to Consul with SystemD, I wanted also to show how to get Consul up and running under Alpine Linux, as it&amp;rsquo;s a little more awkward in some respects.
To start with, I am going to setup Consul as a service - I didn&amp;rsquo;t do this in the Ubuntu version, as there are plenty of useful articles about that already, but that is not the case with Alpine.</description></item><item><title>Configuring Consul DNS Forwarding in Ubuntu 16.04</title><link>https://andydote.co.uk/2019/05/29/consul-dns-forwarding/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/05/29/consul-dns-forwarding/</guid><description>DEPRECATED - This doesn&amp;rsquo;t work properly Please see this post for an updated version which works!
One of the advantages of using Consul for service discovery is that besides an HTTP API, you can also query it by DNS.
The DNS server is listening on port 8600 by default, and you can query both A records or SRV records from it. SRV records are useful as they contain additional properties (priority, weight and port), and you can get multiple records back from a single query, letting you do load balancing client side:</description></item><item><title>Running a Secure RabbitMQ Cluster in Nomad</title><link>https://andydote.co.uk/2019/04/06/nomad-rabbitmq-secure/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/04/06/nomad-rabbitmq-secure/</guid><description>Last time I wrote about running a RabbitMQ cluster in Nomad, one of the main pieces of feedback I received was about the (lack) of security of the setup, so I decided to revisit this, and write about how to launch as secure RabbitMQ node in Nomad.
The things I want to cover are:
Username and Password for the management UI Secure value for the Erlang Cookie SSL for Management and AMQP As usual, the demo repository with all the code is available if you&amp;rsquo;d rather just jump into that.</description></item><item><title>Hyper-V, Docker, and Networking Drama</title><link>https://andydote.co.uk/2019/03/22/hyperv-networking/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/03/22/hyperv-networking/</guid><description>I had a major problem a few hours before giving my Nomad: Kubernetes Without the Complexity talk this morning: the demo stopped working.
Now, the first thing to note is the entire setup of the demo is scripted, and the scripts hadn&amp;rsquo;t changed. The only thing I had done was restart the machine, and now things were breaking.
The Symptoms A docker container started inside the guest VMs with a port mapped to the machine&amp;rsquo;s public IP wasn&amp;rsquo;t resolvable outside the host.</description></item><item><title>RabbitMQ clustering with Consul in Nomad</title><link>https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/</link><pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/</guid><description>Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.
RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i.</description></item><item><title>Testing Immutable Infrastructure</title><link>https://andydote.co.uk/2019/01/01/immutable-infra/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/01/01/immutable-infra/</guid><description>In my previous post, I glossed over one of the most important and useful parts of Immutable Infrastructure: Testability. There are many kinds of tests we can write for our infrastructure, but they should all be focused on the machine/service and maybe it&amp;rsquo;s nearest dependencies, not the entire system.
While this post focuses on testing a full machine (both locally in a VM, and remotely as an Amazon EC2 instance), it is also possible to do most of the same kind of tests against a Docker container.</description></item><item><title>Code-free tracing with LogStash and Jaeger</title><link>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</guid><description>I wanted to show request charts (similar to the network tab in firefox) for requests across our microservices but wanted to do so in the least invasive way possible.
We already use LogStash to collect logs from multiple hosts (via FileBeat) and forward them on to ElasticSearch, so perhaps I can do something to also output from LogStash to a tracing service.
There are a number of tracing services available (AppDash, Jaeger, Zipkin), but unfortunately LogStash doesn&amp;rsquo;t have plugins for any of them or for OpenTracing.</description></item><item><title>Against SemVer</title><link>https://andydote.co.uk/2018/12/16/against-semver/</link><pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/12/16/against-semver/</guid><description>Well, for Applications &amp;amp; Services at least. For libraries, SemVer is the way to go, assuming you can agree on what a breaking change is defined as.
But when it comes to Applications (or SaaS products, websites, etc.) SemVer starts to break down. The problem starts with the most obvious: What is a breaking change? How about a minor change?
What&amp;rsquo;s in a change? For example, if we were to change the UI of a web application, which caused no backend changes, from the user perspective it is probably a breaking change, but not from the developers perspective.</description></item><item><title>Stopping Caring...</title><link>https://andydote.co.uk/2018/12/08/stopping-caring/</link><pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/12/08/stopping-caring/</guid><description>&amp;hellip;about GitHub open source commit streak.
This is, I think, partially triggered by Marc Gravell&amp;rsquo;s post. I currently have had a GitHub commit streak going on 1878 days. The other night I realised, that I don&amp;rsquo;t care about it any more, and more so, I&amp;rsquo;m not sure why I did to start with.
I didn&amp;rsquo;t even mean to start doing it. I just noticed one day that I had done something every day for a couple of weeks, and vaguely wondered how long I could keep that up for.</description></item><item><title>Microservices or Components</title><link>https://andydote.co.uk/2018/10/28/microservices-or-components/</link><pubDate>Sun, 28 Oct 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/10/28/microservices-or-components/</guid><description>One of the reasons people list for using MicroServices is that it helps enforce separation of concerns. This is usually achieved by adding a network boundary between the services. While this is useful, it&amp;rsquo;s not without costs; namely that you&amp;rsquo;ve added a set of new failure modes: the network. We can achieve the same separation of concerns within the same codebase if we put our minds to it. In fact, this is what Simon Brown calls a Modular Monolith, and DHH calls the Majestic Monolith.</description></item><item><title>SketchNotes: Finding Your Service Boundaries</title><link>https://andydote.co.uk/2018/09/10/sketchnotes-finding-service-boundaries/</link><pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/09/10/sketchnotes-finding-service-boundaries/</guid><description>At NDC Oslo this year, I attended Adam Ralph&amp;rsquo;s talk on Finding Your Service Boundaries. I enjoyed it a lot, and once the video came out, I rewatched it, and decided to have a go at doing a “sketchnotes”, which I shared on Twitter, which people liked!
I’ve never done one before, but it was pretty fun. I made it in OneNote, zoomed out a lot, and took a screenshot.</description></item><item><title>Semantic Configuration Validation: Earlier</title><link>https://andydote.co.uk/2018/09/08/semantic-configuration-validation-earlier/</link><pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/09/08/semantic-configuration-validation-earlier/</guid><description>After my previous post on Validating Your Configuration, one of my colleagues made an interesting point, paraphrasing:
I want to know if the configuration is valid earlier than that. At build time preferably. I don&amp;rsquo;t want my service to not start if part of it is invalid.
There are two points here, namely when to validate, and what to do with the results of validation.
Handling Validation Results If your configuration is invalid, you&amp;rsquo;d think the service should fail to start, as it might be configured in a dangerous manner.</description></item><item><title>Feature Toggles with Consul</title><link>https://andydote.co.uk/2018/09/06/consul-feature-toggles/</link><pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/09/06/consul-feature-toggles/</guid><description>Feature Toggles are a great way of helping to deliver working software, although there are a few things which could go wrong. See my talk Feature Toggles: The Good, The Bad and The Ugly for some interesting stories and insights on it!
I was talking with a colleague the other day about how you could go about implementing Feature Toggles in a centralised manner into an existing system, preferably with a little overhead as possible.</description></item><item><title>Validate Your Configuration</title><link>https://andydote.co.uk/2018/08/26/validate-configuration/</link><pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/08/26/validate-configuration/</guid><description>As I have written many times before, your application&amp;rsquo;s configuration should be strongly typed and validated that it loads correctly at startup.
This means not only that the source values (typically all represented as strings) can be converted to the target types (int, Uri, TimeSpan etc) but that the values are semantically valid too.
For example, if you have a web.config file with the following AppSetting, and a configuration class to go with it:</description></item><item><title>Branching and Red Builds</title><link>https://andydote.co.uk/2018/08/10/red-builds/</link><pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/08/10/red-builds/</guid><description>So this is a bit of a rant&amp;hellip;but hopefully with some solutions and workarounds too. So let&amp;rsquo;s kick things off with a nice statement:
I hate broken builds.
So everyone basically agrees on this point I think. The problem is that I mean all builds, including ones on shared feature branches.
Currently, I work on a number of projects which uses small(ish) feature branches. The way this works is that the team agrees on a new feature to work on creates a branch, and then each developer works on tasks, committing on their own branches, and Pull-Requesting to the feature branch.</description></item><item><title>Managing AppSettings in Consul</title><link>https://andydote.co.uk/2018/08/07/managing-consul-appsettings/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/08/07/managing-consul-appsettings/</guid><description>Consul is a great utility to make running your microservice architecture very simple. Amongst other things, it provides Service Discovery, Health Checks, and Configuration. In this post, we are going to be looking at Configuration; not specifically how to read from Consul, but about how we put configuration data into Consul in the first place.
The usual flow for an application using Consul for configuration is as follows:
App Starts Fetches configuration from Consul Configures itself Registers in Consul for Service Discovery Ready Step 2 is very straightforward - you query the local instance of Consul&amp;rsquo;s HTTP API, and read the response into your configuration object (If you&amp;rsquo;re using Microsoft&amp;rsquo;s Configuration libraries on dotnet core, you can use the Consul.</description></item><item><title>Locking Vault Down with Policies</title><link>https://andydote.co.uk/2018/06/23/vault-locking-it-down-with-policies/</link><pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/23/vault-locking-it-down-with-policies/</guid><description>The final part of my Vault miniseries focuses on permissioning, which is provided by Vault&amp;rsquo;s Policies.
As everything in Vault is represented as a path, the policies DSL (Domain Specific Language) just needs to apply permissions to paths to lock things down. For example, to allow all operations on the cubbyhole secret engine, we would define this policy:
path &amp;#34;cubbyhole/*&amp;#34; { capabilities = [&amp;#34;create&amp;#34;, &amp;#34;read&amp;#34;, &amp;#34;update&amp;#34;, &amp;#34;delete&amp;#34;, &amp;#34;list&amp;#34;] } Vault comes with a default policy which allows token operations (such as looking up its own token info, releasing and renewing tokens), and cubbyhole access.</description></item><item><title>Secure Communication with Vault</title><link>https://andydote.co.uk/2018/06/22/vault-secure-communication/</link><pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/22/vault-secure-communication/</guid><description>I think Vault by Hashicorp is a great product - I particularly love how you can do dynamic secret generation (e.g for database connections). But how do you validate that the application requesting the secret is allowed to perform that action? How do you know it&amp;rsquo;s not someone or something impersonating your application?
While musing this at an airport the other day, my colleague Patrik sent me a link to a StackOverflow post about this very question</description></item><item><title>Fixing Docker volume paths on Git Bash on Windows</title><link>https://andydote.co.uk/2018/06/18/git-bash-docker-volume-paths/</link><pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/18/git-bash-docker-volume-paths/</guid><description>My normal development laptop runs Windows, but like a lot of developers, I make huge use of Docker, which I run under Hyper-V. I also heavily use the git bash terminal on windows to work.
Usually, everything works as expected, but I was recently trying to run an ELK (Elasticsearch, Logstash, Kibana) container, and needed to pass in an extra configuration file for Logstash. This caused me a lot of trouble, as nothing was working as expected.</description></item><item><title>Managing Postgres Connection Strings with Vault</title><link>https://andydote.co.uk/2018/06/17/secret-management-vault-postgres-connection/</link><pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/17/secret-management-vault-postgres-connection/</guid><description>One of the points I made in my recent NDC talk on 12 Factor microservices, was that you shouldn&amp;rsquo;t be storing sensitive data, such as API keys, usernames, passwords etc. in the environment variables.
Don&amp;rsquo;t Store Sensitive Data in the Environment
My reasoning is that when you were accessing Environment Variables in Heroku&amp;rsquo;s platform, you were actually accessing some (probably) secure key-value store, rather than actual environment variables.
While you can use something like Consul&amp;rsquo;s key-value store for this, it&amp;rsquo;s not much better as it still stores all the values in plaintext, and has no auditing or logging.</description></item><item><title>Writing Conference Talks</title><link>https://andydote.co.uk/2018/05/15/writing-conference-talks/</link><pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/05/15/writing-conference-talks/</guid><description>I saw an interesting question on twitter today:
Hey, people who talk at things: How long does it take you to put a new talk together?
I need like 50 hours over at least a couple of months to make something I don&amp;rsquo;t hate. I&amp;rsquo;m trying to get that down (maybe by not doing pictures?) but wondering what&amp;rsquo;s normal for everyone else.
Source
I don&amp;rsquo;t know how long it takes me to write a talk - as it is usually spread over many weeks/months, worked on as and when I have inspiration.</description></item><item><title>Test Expressiveness</title><link>https://andydote.co.uk/2018/02/26/test-expressiveness/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/02/26/test-expressiveness/</guid><description>We have a test suite at work which tests a retry decorator class works as expected. One of the tests checks that when the inner implementation throws an exception, it will log the number of times it has failed:
[Test] public async Task ShouldLogRetries() { var mockClient = Substitute.For&amp;lt;IContractProvider&amp;gt;(); var logger = Subsitute.For&amp;lt;ILogger&amp;gt;(); var sut = new RetryDecorator(mockClient, logger, maxRetries: 3); mockClient .GetContractPdf(Arg.Any&amp;lt;string&amp;gt;()) .Throws(new ContractDownloadException()); try { await sut.GetContractPdf(&amp;#34;foo&amp;#34;); } catch (Exception e){} logger.</description></item><item><title>Task Chaining and the Pipeline Operator</title><link>https://andydote.co.uk/2018/02/20/task-chainging-pipeline-operator/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/02/20/task-chainging-pipeline-operator/</guid><description>Since I have been trying to learn a functional language (Elixir), I have noticed how grating it is when in C# I need to call a few methods in a row, passing the results of one to the next.
The bit that really grates is that it reads backwards, i.e. the rightmost function call is invoked first, and the left hand one last, like so:
await WriteJsonFile(await QueueParts(await ConvertToModel(await ReadBsxFile(record)))); In Elixir (or F# etc.</description></item><item><title>Tweaking Processes to Remove Errors</title><link>https://andydote.co.uk/2017/12/09/tweaking-process-remove-errors/</link><pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/12/09/tweaking-process-remove-errors/</guid><description>When we are developing (internal) Nuget packages at work, the process used is the following:
Get latest of master New branch feature-SomethingDescriptive Implement feature Push to GitHub TeamCity builds Publish package to the nuget feed Pull request Merge to master Obviously 3 to 6 can repeat many times if something doesn&amp;rsquo;t work out quite right.
There are a number of problems with this process:
Pull-request after publishing Pull requests are a great tool which we use extensively, but in this case, they are being done too late.</description></item><item><title>Evolutionary Development</title><link>https://andydote.co.uk/2017/11/17/evolutionary-development/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/11/17/evolutionary-development/</guid><description>Having recently finished reading the Building Evolutionary Architectures: Support Constant Change book, I got to thinking about a system which was fairly representative of an architecture which was fine for it&amp;rsquo;s initial version, but it&amp;rsquo;s usage had outgrown the architecture.
Example System: Document Storage The system in question was a file store for a multi user, internal, desktop based CRM system. The number of users was very small, and the first implementation was just a network file share.</description></item><item><title>Strong Configuration Composition</title><link>https://andydote.co.uk/2017/11/09/configuration-composition/</link><pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/11/09/configuration-composition/</guid><description>It&amp;rsquo;s no secret I am a fan of strong typing - not only do I talk and blog about it a lot, but I also have a library called Stronk which provides strong typed configuration for non dotnet core projects.
The problem I come across often is large configurations. For example, given the following project structure (3 applications, all reference the Domain project):
DemoService `-- src |-- Domain | |-- Domain.</description></item><item><title>Alarm Fatigue</title><link>https://andydote.co.uk/2017/10/30/alarm-fatigue/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/10/30/alarm-fatigue/</guid><description>I&amp;rsquo;ve been on-call for work over the last week for the first time, and while it wasn&amp;rsquo;t as alarming (heh) as I thought it might be, I have had a few thoughts on it.
Non-action Alarms We have an alarm periodically about an MVC View not getting passed the right kind of model. The resolution is to mark the bug as completed/ignored in YouTrack. Reading the stack trace, I can see that the page is expecting a particular model, but is being given a HandleErrorInfo model, which is an in built type.</description></item><item><title>Vagrant in the world of Docker</title><link>https://andydote.co.uk/2017/10/22/vagrant-in-a-world-of-docker/</link><pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/10/22/vagrant-in-a-world-of-docker/</guid><description>I gave a little talk at work recently on my use of Vagrant, what it is, and why it is still useful in a world full of Docker containers.
So, What is Vagrant? Vagrant is a product by Hashicorp, and is for scripting the creation of (temporary) virtual machines. It&amp;rsquo;s pretty fast to create a virtual machine with too, as it creates them from a base image (known as a &amp;ldquo;box&amp;rdquo;.</description></item><item><title>Testing RabbitMQ Concurrency in MassTransit</title><link>https://andydote.co.uk/2017/10/11/masstransit-rabbitmq-concurrency-testing/</link><pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/10/11/masstransit-rabbitmq-concurrency-testing/</guid><description>We have a service which consumes messages from a RabbitMQ queue - for each message, it makes a few http calls, collates the results, does a little processing, and then pushes the results to a 3rd party api. One of the main benefits to having this behind a queue is our usage pattern - the queue usually only has a few messages in it per second, but periodically it will get a million or so messages within 30 minutes (so from ~5 messages/second to ~560 messages/second.</description></item><item><title>Composite Decorators with StructureMap</title><link>https://andydote.co.uk/2017/10/04/structuremap-composite-decorator/</link><pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/10/04/structuremap-composite-decorator/</guid><description>While I was developing my Crispin project, I ended up needing to create a bunch of implementations of a single interface, and then use all those implementations at once (for metrics logging).
The interface looks like so:
public interface IStatisticsWriter { Task WriteCount(string format, params object[] parameters); } And we have a few implementations already:
LoggingStatisticsWriter - writes to an ILogger instance StatsdStatisticsWriter - pushes metrics to StatsD InternalStatisticsWriter - aggregates metrics for exposing via Crispin&amp;rsquo;s api To make all of these be used together, I created a fourth implementation, called CompositeStatisticsWriter (a name I made up, but apparently matches the Gang of Four definition of a composite!</description></item><item><title>Integration Testing with Dotnet Core, Docker and RabbitMQ</title><link>https://andydote.co.uk/2017/10/02/dotnet-core-docker-integration-tests/</link><pubDate>Mon, 02 Oct 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/10/02/dotnet-core-docker-integration-tests/</guid><description>When building libraries, not only is it a good idea to have a large suite of Unit Tests, but also a suite of Integration Tests.
For one of my libraries (RabbitHarness) I have a set of tests which check it behaves as expected against a real instance of RabbitMQ. Ideally these tests will always be run, but sometimes RabbitMQ just isn&amp;rsquo;t available such as when running on AppVeyor builds, or if I haven&amp;rsquo;t started my local RabbitMQ Docker container.</description></item><item><title>Implementing Custom Aspnet Core ModelBinders</title><link>https://andydote.co.uk/2017/09/22/implemeting-custom-aspnetcore-modelbinders/</link><pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/09/22/implemeting-custom-aspnetcore-modelbinders/</guid><description>This post is a summary of a stream I did last night where I implemented all of this. If you want to watch me grumble my way through it, it&amp;rsquo;s available on YouTube here.
In my Crispin project, I wanted the ability to support loading Toggles by both name and ID, for all operations. As I use mediator to send messages from my controllers to the handlers in the domain, this means that I had to either:</description></item><item><title>Testing Containers or Test Behaviour, Not Implementation</title><link>https://andydote.co.uk/2017/09/17/testing-containers/</link><pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/09/17/testing-containers/</guid><description>The trouble with testing containers is that usually the test ends up very tightly coupled to the implementation.
Let&amp;rsquo;s see an example. If we start off with an interface and implementation of a &amp;ldquo;cache&amp;rdquo;, which in this case is just going to store a single string value.
public interface ICache { string Value { get; set; } } public class Cache { public string Value { get; set; } } We then setup our container (StructureMap in this case) to return the same instance of the cache whenever an ICache is requested:</description></item><item><title>Repositories Revisited (and why CQRS is better)</title><link>https://andydote.co.uk/2017/09/09/repositories-revisited/</link><pubDate>Sat, 09 Sep 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/09/09/repositories-revisited/</guid><description>TLDR: I still don&amp;rsquo;t like Repositories!
Recently I had a discussion with a commenter on my The problems with, and solutions to Repositories post, and felt it was worth expanding on how I don&amp;rsquo;t use repositories.
My applications tend to use the mediator pattern to keep things decoupled (using the Mediatr library), and this means that I end up with &amp;ldquo;handler&amp;rdquo; classes which process messages; they load something from storage, call domain methods, and then write it back to storage, possibly returning some or all the data.</description></item><item><title>Serilog LogContext with StructureMap and SimpleInjector</title><link>https://andydote.co.uk/2017/07/28/serilog-context-with-structuremap-and-simpleinjector/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/07/28/serilog-context-with-structuremap-and-simpleinjector/</guid><description>This article has been updated after feedback from .Net Junkie (Godfather of SimpleInjector). I now have a working SimpleInjector implementation of this, and am very appreciative of him for taking the time to help me :)
Serilog is one of the main set of libraries I use on a regular basis, and while it is great at logging, it does cause something in our codebase that I am less happy about.</description></item><item><title>Getting Things Done</title><link>https://andydote.co.uk/2017/07/15/getting-things-done/</link><pubDate>Sat, 15 Jul 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/07/15/getting-things-done/</guid><description>I have been trying to actually be productive in my evenings and weekends, but I find I often end up not getting as much done as I feel I could have. I end up browsing imgur, reading slashdot, reddit, twitter, etc. rather than reading books, writing or anything else.
The first point doesn&amp;rsquo;t fit in anywhere else, but somewhere I saw a tip about keeping a house clean (I think):</description></item><item><title>Terraform, Kinesis Streams, Lambda and IAM problems</title><link>https://andydote.co.uk/2017/07/12/terraform-kinesis-lambda-iam/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/07/12/terraform-kinesis-lambda-iam/</guid><description>I hit an problem the recently with Terraform, when I was trying to hook up a Lambda Trigger to a Kinesis stream. Both the lambda itself, and the stream creation succeeded within Terraform, but the trigger would just stay stuck on &amp;ldquo;creating&amp;hellip;&amp;rdquo; for at least 5 minutes, before I got bored of waiting and killed the process. Several attempts at doing this had the same issue.
The code looked something along the lines of this:</description></item><item><title>S3 Multi-File upload with Terraform</title><link>https://andydote.co.uk/2017/04/23/s3-multi-file-upload-terraform/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/04/23/s3-multi-file-upload-terraform/</guid><description>Hosting a static website with S3 is really easy, especially from terraform:
First off, we want a public readable S3 bucket policy, but we want to apply this only to one specific bucket. To achive that we can use Terraform&amp;rsquo;s template_file data block to merge in a value:
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Sid&amp;#34;: &amp;#34;PublicReadGetObject&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Action&amp;#34;: [ &amp;#34;s3:GetObject&amp;#34; ], &amp;#34;Resource&amp;#34;: [ &amp;#34;arn:aws:s3:::${bucket_name}/*&amp;#34; ] } ] } As you can see the interpolation syntax is pretty much the same as how you use variables in terraform itself.</description></item><item><title>Don't write Frameworks, write Libraries</title><link>https://andydote.co.uk/2017/04/16/dont-write-frameworks-write-libraries/</link><pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/04/16/dont-write-frameworks-write-libraries/</guid><description>Programmers have a fascination with writing frameworks for some reason. There are many problems with writing frameworks:
Opinions Frameworks are opinionated, and will follow their author&amp;rsquo;s opinions on how things should be done, such as application structure, configuration, and methodology. The problem this gives is that not everyone will agree with the author, or their framework&amp;rsquo;s opinions. Even if they really like part of how the framework works, they might not like another part, or might not be able to rewrite their application to take advantage of the framework.</description></item><item><title>Using Terraform to setup AWS API-Gateway and Lambda</title><link>https://andydote.co.uk/2017/03/17/terraform-aws-lambda-api-gateway/</link><pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/03/17/terraform-aws-lambda-api-gateway/</guid><description>I have been writing simple webhook type applications using Claudiajs, which in behind the scenes is using Aws&amp;rsquo;s Lambda and Api Gateway to make things happen, but I really wanted to understand what exactly it was doing for me, and how I could achieve the same results using Terraform.
The Lambda Function I started off with a simple NodeJS function, in a file called index.js
exports.handler = function(event, context, callback) { callback(null, { statusCode: &amp;#39;200&amp;#39;, body: JSON.</description></item><item><title>Unit Tests &amp; Scratchpads</title><link>https://andydote.co.uk/2017/01/21/unit-tests-and-scratchpads/</link><pubDate>Sat, 21 Jan 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/01/21/unit-tests-and-scratchpads/</guid><description>Often when developing something, I have the need to check how a function or library works. For example, I always have to check for this question:
Does Directory.ListFiles(&amp;quot;.\\temp\\&amp;quot;) return a list of filenames, a list of relative filepaths, or a list of rooted filepaths?
It returns relative filepaths by the way:
Directory.ListFiles(&amp;#34;.\\temp\\&amp;#34;); [ &amp;#34;.\temp\NuCrunch.Tests.csproj&amp;#34;, &amp;#34;.\temp\packages.config&amp;#34;, &amp;#34;.\temp\Scratchpad.cs&amp;#34; ] Now that there is a C# Interactive window in Visual Studio, you can use that to test the output.</description></item><item><title>Update all Docker images</title><link>https://andydote.co.uk/2017/01/16/update-all-docker-images/</link><pubDate>Mon, 16 Jan 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/01/16/update-all-docker-images/</guid><description>My work&amp;rsquo;s wifi is much faster than my 4G connection, so periodically I want to update all my docker images on my personal laptop while at work.
As I want to just set it going and then forget about it, I use the following one liner to do a docker pull against each image on my local machine:
docker images | grep -v REPOSITORY | awk &amp;#39;{print $1}&amp;#39;| xargs -L1 docker pull If you only want to fetch the versions you have the tags for:</description></item><item><title>MediatR and Magic</title><link>https://andydote.co.uk/2017/01/07/mediatr-and-magic/</link><pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/01/07/mediatr-and-magic/</guid><description>Having recently watched Greg Young&amp;rsquo;s excellent talk on 8 Lines of Code I was thinking about how this kind of thinking applies to the mediator pattern, and specifically the MediatR implementation.
I have written about the advantages of CQRS with MediatR before, but having used it for a long time now, there are some parts which cause friction on a regular basis.
The problems Discoverability The biggest issue first. You have a controller with the following constructor:</description></item><item><title>Git Aliases</title><link>https://andydote.co.uk/2017/01/06/git-aliases/</link><pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2017/01/06/git-aliases/</guid><description>Git is great, but creating some git aliases is a great way to make your usages even more efficient.
To add any of these you can either copy and paste into the [alias] section of your .gitconfig file or run git config --global alias.NAME 'COMMAND' replacing NAME with the alias to use, and COMMAND with what to run.
So without further ado, here are the ones I have created and use on a very regular basis.</description></item><item><title>Strong Type All The Configurations</title><link>https://andydote.co.uk/2016/12/06/strong-type-all-the-configurations/</link><pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/12/06/strong-type-all-the-configurations/</guid><description>As anyone I work with can attest, I a have been prattling on about strong typing everything for quite a while. One of the places I feel people don&amp;rsquo;t utilise strong typing enough is in application configuration. This manifests in a number of problems in a codebase.
The Problems The first problem is when nothing at all is done about it, and you end up with code spattered with this:</description></item><item><title>Shouldly: Why would you assert any other way?</title><link>https://andydote.co.uk/2016/10/09/assertion-style/</link><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/10/09/assertion-style/</guid><description>I like to make my development life as easy as possible - and removing small irritations is a great way of doing this. Having used Shouldly in anger for a long time, I have to say I feel a little hamstrung when going back to just using NUnit&amp;rsquo;s assertions.
I have been known on a couple of projects which use only NUnit assertions, when trying to solve a test failure with array differences, to install Shouldly, fix the test, then remove Shouldly again!</description></item><item><title>Visualising NuGet Dependencies</title><link>https://andydote.co.uk/2016/09/12/nuget-dependencies/</link><pubDate>Mon, 12 Sep 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/09/12/nuget-dependencies/</guid><description>My new place of work has a lot of nuget packages, and I wanted to understand the dependencies between them. To do this I wrote a simple shell script to find all the packages.config files on my machine, and output all the relationships in a way which I could view them.
The format for viewing I use for this is Graphviz&amp;rsquo;s dot language, and the resulting output can be pasted into WebGraphviz to view.</description></item><item><title>Preventing MicroService Boilerplate</title><link>https://andydote.co.uk/2016/07/17/preventing-microservice-boilerplate/</link><pubDate>Sun, 17 Jul 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/07/17/preventing-microservice-boilerplate/</guid><description>One of the downsides to microservices I have found is that I end up repeating the same blocks of code over and over for each service. Not only that, but the project setup is repetitive, as all the services use the Single Project Service and Console method.
What do we do in every service? Initialise Serilog. Add a Serilog sink to ElasticSearch for Kibana (but only in non-local config.) Hook/Unhook the AppDomain.</description></item><item><title>Database Integrations for MicroServices</title><link>https://andydote.co.uk/2016/06/09/database-integrations-for-microservices/</link><pubDate>Thu, 09 Jun 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/06/09/database-integrations-for-microservices/</guid><description>This is a follow up post after seeing Michal Franc&amp;rsquo;s NDC talk on migrating from Monolithic architectures.
One point raised was that Database Integration points are a terrible idea - and I wholeheartedly agree. However, there can be a number of situations where a Database Integration is the best or only way to achieve the end goal. This can be either technical; say a tool does not support API querying (looking at you SSRS), or cultural; the other team either don&amp;rsquo;t have the willingness, time, or power to learn how to query an API.</description></item><item><title>CQS with Mediatr</title><link>https://andydote.co.uk/2016/03/19/cqs-with-mediatr/</link><pubDate>Sat, 19 Mar 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/03/19/cqs-with-mediatr/</guid><description>This article is some extra thoughts I had on api structure after reading Derek Comartin.
Asides from the benefits that Derek mentions (no fat repositories, thin controllers), there are a number of other advantages that this style of architecture brings.
Ease of Testing By using Command and Queries, you end up with some very useful seams for writing tests.
For controllers With controllers, you typically use Dependency injection to provide an instance of IMediator:</description></item><item><title>RabbitMQ integration tests in XUnit</title><link>https://andydote.co.uk/2016/03/18/rabbitmq-xunit/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/03/18/rabbitmq-xunit/</guid><description>Quite a number of my projects involve talking to RabbitMQ, and to help check things work as expected, I often have a number of integration tests which talk to a local RabbitMQ instance.
While this is fine for tests being run locally, it does cause problems with the build servers - we don&amp;rsquo;t want to install RabbitMQ on there, and we don&amp;rsquo;t typically want the build to be dependent on RabbitMQ.</description></item><item><title>Generating AssemblyInfo files with Gulp</title><link>https://andydote.co.uk/2015/11/19/generating-assemblyinfo-files-gulpjs/</link><pubDate>Thu, 19 Nov 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/11/19/generating-assemblyinfo-files-gulpjs/</guid><description>When changing a project&amp;rsquo;s build script over to Gulpjs, I ran into a problem with one step - creating an AssemblyInfo.cs file.
My projects have their version number in the package.json file, and I read that at compile time, pull in some information from the build server, and write that to an AssemblyVersion.cs file. This file is not tracked by git, and I don&amp;rsquo;t want it showing up as a modification if you run the build script locally.</description></item><item><title>Posting PlainText to Asp WebApi</title><link>https://andydote.co.uk/2015/09/21/webapi-post-plaintext/</link><pubDate>Mon, 21 Sep 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/09/21/webapi-post-plaintext/</guid><description>Recently I have been writing a WebApi project which needs to accept plaintext via the body of a PUT request, and did the logical thing of using the FromBodyAttribute
public HttpStatusCode PutKv([FromBody]string content, string keyGreedy) { return HttpStatusCode.OK; } Which didn&amp;rsquo;t work, with the useful error message of &amp;ldquo;Unsupported media type.&amp;rdquo;
It turns out that to bind a value type with the FromBody attribute, you have to prefix the body of your request with an =.</description></item><item><title>Running pre-compiled microservices in Docker with Mono</title><link>https://andydote.co.uk/2015/09/15/pre-compiled-microservices/</link><pubDate>Tue, 15 Sep 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/09/15/pre-compiled-microservices/</guid><description>Last time we went through creating a Dockerfile for a microservice, with the service being compiled on creation of the container image, using xbuild.
However we might not want to compile the application to create the container image, and use an existing version (e.g. one created by a build server.)
Our original Dockerfile was this:
FROM mono:3.10-onbuild RUN apt-get update &amp;amp;&amp;amp; apt-get install mono-4.0-service -y CMD [ &amp;#34;mono-service&amp;#34;, &amp;#34;./MicroServiceDemo.exe&amp;#34;, &amp;#34;--no-daemon&amp;#34; ] EXPOSE 12345 We only need to make a few modifications to use a pre-compiled application:</description></item><item><title>Running microservices in Docker with Mono</title><link>https://andydote.co.uk/2015/09/05/running-microservices-in-docker-with-mono/</link><pubDate>Sat, 05 Sep 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/09/05/running-microservices-in-docker-with-mono/</guid><description>Getting a service running under Docker is fairly straight forward once you have all the working parts together. I have an app written (following my guide on service and console in one), which uses Owin to serve a web page as a demo:
install-package Microsoft.Owin.SelfHost public partial class Service : ServiceBase { //see the service console post for the rest of this protected override void OnStart(string[] args) { _app = WebApp.</description></item><item><title>A single project Windows Service and Console</title><link>https://andydote.co.uk/2015/08/30/single-project-service-and-console/</link><pubDate>Sun, 30 Aug 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/08/30/single-project-service-and-console/</guid><description>I have found that when developing MicroServices, I often want to run them from within Visual Studio, or just as a console application, and not have to bother with the hassle of installing as windows services.
In the past I have seen this achieved by creating a Class Library project with all the actual implementation inside it, and then both a Console Application and Windows Service project referencing the library and doing nothing other than calling a .</description></item><item><title>Don't Let The Database Dictate Your Design</title><link>https://andydote.co.uk/2015/04/01/dont-let-the-database-dictate-your-design/</link><pubDate>Wed, 01 Apr 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/04/01/dont-let-the-database-dictate-your-design/</guid><description>I have been thinking recently about how the database can influence our design decisions, and perhaps makes them harder than they need to be in some cases. An example of this is the design of a system which stores data about people, specifically for this, their email addresses. A cut down version of the structure is this:
table people id serial primary key firstname varchar(50) lastname varchar(50) table emails id serial primary key person_id int =&amp;gt; people.</description></item><item><title>The problems with and solutions to Repositories</title><link>https://andydote.co.uk/2015/03/28/problems-with-and-solutions-to-repositories/</link><pubDate>Sat, 28 Mar 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/03/28/problems-with-and-solutions-to-repositories/</guid><description>Repositories are a design pattern which I have never been a huge fan of. I can see the use of them as a good layer boundary, but too often I see them being used all over the place instead of at an infrastructure level in a code base.
A particularly prevalent version of this misuse I see is self populating collections. These generally inherit List&amp;lt;TEntity&amp;gt; or Dictionary&amp;lt;TID, TEntity&amp;gt;, and provide a set of methods such as .</description></item><item><title>Communicating Intent in APIs</title><link>https://andydote.co.uk/2015/03/25/communicating-intent-in-apis/</link><pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/03/25/communicating-intent-in-apis/</guid><description>Recently was trying to work out how to allow custom resources to be specified in Dashen. I already know what data is needed/defined for a resource: a name, a MIME type, and a Stream. We can make this required data known very easily:
public class Resource { public string Name { get; private set; } public string MimeType { get; private set; } public Stream Content { get; private set; } public Resource(string name, string mimeType, Stream content) { Name = name; MimeType = mimeType; Content = content; } } As all the parameters can only be set through the constructor, you are communicating that they are all required.</description></item><item><title>Encapsulation in Warcraft Addons - Inheritance</title><link>https://andydote.co.uk/2014/12/05/encapsulation-in-warcraft-addons-inheritance/</link><pubDate>Fri, 05 Dec 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/12/05/encapsulation-in-warcraft-addons-inheritance/</guid><description>Using Inheritance (sort of) When we actually need inheritance, things get a little more complicated. We need to use two of lua&amp;rsquo;s slightly harder features to get it to work: metatables and colon notation. A little background on these will help:
MetaTables All &amp;ldquo;objects&amp;rdquo; in lua are tables, and tables can something called a metatable added to them. Metatables can have special methods on them which run under certain circumstances (called metamethods), such as keys being added.</description></item><item><title>Encapsulation in Warcraft Addons - Closures</title><link>https://andydote.co.uk/2014/11/28/encapsulation-in-warcraft-addons-closures/</link><pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/11/28/encapsulation-in-warcraft-addons-closures/</guid><description>In the last post I alluded to the fact that if you put in a little leg work, you could write well encapsulated objects in lua. There are two main ways to do this; with closures, and with metatables. In this post we will deal with using closures, and in the next post we will cover using metatables.
Using Closures The simplest way to write an object in lua is with a closure to hide all the variables from the outside world.</description></item><item><title>Good Design in Warcraft Addons/Lua</title><link>https://andydote.co.uk/2014/11/23/good-design-in-warcraft-addons/</link><pubDate>Sun, 23 Nov 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/11/23/good-design-in-warcraft-addons/</guid><description>Lack of Encapsulation in Addons I first noticed a lack of good design in addon code when I started trying to tweak existing addons to be slightly different.
One of the stand out examples was a Threat Meter (you know which one I mean). It works well, but I felt like writing my own, to make it really fit into my UI, with as little overhead as possible. Not knowing how to even begin writing a Threat Meter, I downloaded a copy, and opened its source directory&amp;hellip; to discover that the entire addon is one 3500+ line file, and 16 Ace.</description></item><item><title>Edge.js for Embedded Webuis</title><link>https://andydote.co.uk/2014/08/04/edgejs-for-embedded-webuis/</link><pubDate>Mon, 04 Aug 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/08/04/edgejs-for-embedded-webuis/</guid><description>We work we have a number of windows services which each have a lot of stats they could expose. Currently they are only interrogatable by the logfiles and from any notifications we receive.
I have been toying with the idea of hosting a website in-process which would give a simple dashboard ui and access to a live view of the log file. The idea first struck me when I was experimenting with FubuMvc, as they have an EmbeddedFubuMvcServer, which is very easy to use:</description></item><item><title>Configuring Dapper to work with custom types</title><link>https://andydote.co.uk/2014/07/22/configuring-dapper-to-work-with-custom-types/</link><pubDate>Tue, 22 Jul 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/07/22/configuring-dapper-to-work-with-custom-types/</guid><description>In the last post we looked at using custom ID types to help abstract the column type from the domain.
This works well until you start trying to load and save entities using an ORM, as the ORM has not way to know how to map a column to a custom type. ORMs provide extension points to allow you to create these mappings. As I tend to favour using Dapper, we will go through setting it up to work with our custom ID types.</description></item><item><title>Strong Type your entity IDs.</title><link>https://andydote.co.uk/2014/07/17/strong-type-your-entity-ids/</link><pubDate>Thu, 17 Jul 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/07/17/strong-type-your-entity-ids/</guid><description>The Database is just an Implementation Detail A quote from Martin Fowler given during his Architecture talk stated that the Database in your application should just be an implementation detail. I agree on this wholeheartedly and find that its really not that difficult to achieve if you think about your architecture carefully.
Having said that, I still see parts of the database implementation leaking out into the domain, mainly in the form of IDs.</description></item><item><title>Specific Interfaces</title><link>https://andydote.co.uk/2014/06/08/specific-interfaces-smaller-abstractions/</link><pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/06/08/specific-interfaces-smaller-abstractions/</guid><description>While writing my CruiseCli project, I needed to do some data storage, and so used my standard method of filesystem access, the IFileSystem. This is an interface and implementation which I tend to copy from project to project, and use as is. The interface looks like the following:
public interface IFileSystem { bool FileExists(string path); void WriteFile(string path, Stream contents); void AppendFile(string path, Stream contents); Stream ReadFile(string path); void DeleteFile(string path); bool DirectoryExists(string path); void CreateDirectory(string path); IEnumerable&amp;lt;string&amp;gt; ListDirectory(string path); void DeleteDirectory(string path); } And the standard implementation looks like the following:</description></item><item><title>Using StructureMap Registries for better separation</title><link>https://andydote.co.uk/2014/05/19/using-structuremap-registries/</link><pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/05/19/using-structuremap-registries/</guid><description>When it comes to configuring StructureMap, it supports the use of Registries. Registries support everything that the standard configure method does(new Container(c =&amp;gt; { /* */});).
There are two main reasons that I use the registries rather then doing all my configuration in the Container&amp;rsquo;s lambda: separation of concerns (one registry per area of code) and easier testing (which we will go into shortly).
The only down side I can see to using registries is that it can scatter your configuration across your codebase - but if you have ReSharper, doing a &amp;lsquo;Find Implementations&amp;rsquo; on Registry will find them all for you, so it really isn&amp;rsquo;t much of a down side.</description></item><item><title>Writing Rich Domain Models</title><link>https://andydote.co.uk/2014/05/04/rich-domain-modeling/</link><pubDate>Sun, 04 May 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/05/04/rich-domain-modeling/</guid><description>The term Rich Domain Model is used to describe a domain model which really shows you how you should be using and manipulating the model, rather than letting you do anything with it. It is the opposite of an Anaemic Domain Model, which provides a very low abstraction over the data storage (generally), but with little to no enforcing of rules.
The Anaemic Domain Model To take the standard model of a person who has addresses and phone numbers etc seems a little contrite, so lets run through an example using timesheets (bear in mind I don&amp;rsquo;t know what really goes into a timesheet system, this just seems reasonable).</description></item><item><title>Using a Micro ORM to decouple your DB Access</title><link>https://andydote.co.uk/2014/03/29/using-a-micro-orm-to-decouple-your-db-access/</link><pubDate>Sat, 29 Mar 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/03/29/using-a-micro-orm-to-decouple-your-db-access/</guid><description>One of the databases I use on a regular bases has a rather interesting column naming scheme; all columns have a prefix, based on the table name. For example, the table containing people would have the prefix PEO_, so you would have this:
Select * from People PEO_PersonID, PEO_FirstName, PEO_LastName, PEO_DoB ----------------------------------------------------- 1 John Jones 1984-07-15 I believe the idea was so that when querying, you would not have any column name clashes.</description></item><item><title>SOLID Principles - DIP</title><link>https://andydote.co.uk/2014/03/15/solid-principles-dip/</link><pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/03/15/solid-principles-dip/</guid><description>Single Responsibility | Open Closed | Liskov Substitution | Interface Segregation | Dependency Inversion
The Dependency Inversion Principle states that &amp;ldquo;Depend upon Abstractions. Do not depend upon concretions&amp;rdquo;. A good real world example of this is plug sockets around your house; any device you buy can be plugged into any socket in your house. You don&amp;rsquo;t have to buy new set of devices when you move house, and you don&amp;rsquo;t have to buy a new house for your devices!</description></item><item><title>SOLID Principles - ISP</title><link>https://andydote.co.uk/2014/03/01/solid-principles-isp/</link><pubDate>Sat, 01 Mar 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/03/01/solid-principles-isp/</guid><description>Interface Segregation Principle Single Responsibility | Open Closed | Liskov Substitution | Interface Segregation | Dependency Inversion
Interface Segregation I find is often ignored, or people tend not to see the point in. Segregating your Interfaces is a very useful way of reducing compexity in your systems, and comes with a number of benefits, such as making mocking inputs easier, and making your objects smaller and simpler.
So as usual, lets start off with an set of types which don&amp;rsquo;t adhere to the principle.</description></item><item><title>SOLID Principles - LSP</title><link>https://andydote.co.uk/2014/02/23/solid-principles-lsp/</link><pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/02/23/solid-principles-lsp/</guid><description>Liskov Substitution Principle Single Responsibility | Open Closed | Liskov Substitution | Interface Segregation | Dependency Inversion
The Liskov Substitution Principle is states:
If S is a sub-type of T, then objects of type T maybe replaced with objects of type S
At face value, it means that a small class hierarchy like this:
public class FileEntry { } public class DbFileEntry : FileEntry { } And a method which takes in a FileEntry, can be called like this:</description></item><item><title>SOLID Principles - OCP</title><link>https://andydote.co.uk/2014/02/19/solid-principles-ocp/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/02/19/solid-principles-ocp/</guid><description>Open Closed Principle Single Responsibility | Open Closed | Liskov Substitution | Interface Segregation | Dependency Inversion
The Open Closed Principle is one that I often find is miss-understood - how can something be open for extension, but closed for modification? A good example of this principle being implemented cropped up at work a while ago, we had a UI element which has a reusable grid, which gets populated with data based on a menu selection.</description></item><item><title>SOLID Principles - SRP</title><link>https://andydote.co.uk/2014/02/18/solid-principles-srp/</link><pubDate>Tue, 18 Feb 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/02/18/solid-principles-srp/</guid><description>Single Responsibility Principle Single Responsibility | Open Closed | Liskov Substitution | Interface Segregation | Dependency Inversion
SRP (Single Responsibility Principle) is something I hear a lot of developers agree is a good thing, but when I read their code, they violate it without realising, or don&amp;rsquo;t see the use in their particular case.
A particularly prominent example I find in our code bases is Permissioning and Caching. These two requirements can often slip into classes slowly - especially if requirements are not clear, or change as the task progresses.</description></item><item><title>Specialising a General Application</title><link>https://andydote.co.uk/2014/02/02/specialising-a-general-application/</link><pubDate>Sun, 02 Feb 2014 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2014/02/02/specialising-a-general-application/</guid><description>Currently our application at work is used by all employees - sales staff, legal team, marketing, accounts etc. This means we have one very large, and general fit application. It covers everyone&amp;rsquo;s needs just, and the largest group of users (sales in this case) have an application which closely matches what they need. This is at the expense of the other teams having an application that is not quite right - close, but could be better.</description></item><item><title>Analysis of Frames in World of Warcraft</title><link>https://andydote.co.uk/2013/11/17/analasys-of-frames-in-world-of-warcraft/</link><pubDate>Sun, 17 Nov 2013 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2013/11/17/analasys-of-frames-in-world-of-warcraft/</guid><description>In this post we will be looking at how the Frame and associated objects are (probably) constructed behind the scenes. This will all be done via inspection in lua from the games scripting engine.
The basic display item in Warcraft is the Frame. Frames are not only use for displaying data, but used to listen to events in the background. Another interesting characteristic of a Frame is that you cannot destroy them.</description></item><item><title>Creating a FubuMvc website</title><link>https://andydote.co.uk/2013/08/26/creating-a-fubumvc-website/</link><pubDate>Mon, 26 Aug 2013 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2013/08/26/creating-a-fubumvc-website/</guid><description>Add new Empty Web Application to your solution PM&amp;gt; Install-package fubumvc Add folder Features Add folder Features\Home Add Features\Home\HomeInputModel.cs Add Features\Home\HomeViewModel.cs Add Features\Home\HomeEndpoint.cs Add Features\Home\Home.spark Setup application (ConfigureFubuMVC.cs) Actions.FindBy(x =&amp;gt; { x.Applies.ToThisAssembly(); x.IncludeClassesSuffixedWithEndpoint(); }); Routes.HomeIs&amp;lt;HomeInputModel&amp;gt;(); Routes.ConstrainToHttpMethod(x =&amp;gt; x.Method.Name.Equals(&amp;#34;Get&amp;#34;, StringComparison.OrdinalIgnoreCase), &amp;#34;GET&amp;#34;); Routes.IgnoreControllerNamespaceEntirely(); //removes /features/home/ from the start of urls Routes.IgnoreMethodSuffix(&amp;#34;Get&amp;#34;); //removes the trailing /get from our urls HomeViewModel.cs: public String Message { get; set; } HomeEndpoint.cs: public HomeViewModel Get(HomeInputModel input) { return new HomeViewModel { Message = &amp;#34;Dave&amp;#34; }; } Home.</description></item><item><title>Checking a Type for an Attribute</title><link>https://andydote.co.uk/2012/11/02/checking-a-type-for-an-attribute/</link><pubDate>Fri, 02 Nov 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/11/02/checking-a-type-for-an-attribute/</guid><description>I needed to be able to detect at run time if an Enum has a specific Attribute on it. Generalizing it, I came up with this:
Calling:
var hasFlags = typeof(EnumWithFlags).HasAttribute&amp;lt;FlagsAttribute&amp;gt;(); Implementation:
public static Boolean HasAttribute&amp;lt;T&amp;gt;(this Type self) where T : Attribute { if (self == null) { throw new ArgumentNullException(&amp;#34;self&amp;#34;); } return self.GetCustomAttributes(typeof(T), false).Any(); } It may only be two lines, but it is very useful none the less.</description></item><item><title>SqlDataReader.HasRows Problems</title><link>https://andydote.co.uk/2012/10/30/sqldatareaderhasrows-problems/</link><pubDate>Tue, 30 Oct 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/10/30/sqldatareaderhasrows-problems/</guid><description>For the last 6 years or so at work, we have had an intermittent bug. In this case, intermittent means around once in 6 months or so. A little background to the problem first:
Our data access is done via what was originally Microsoft&amp;rsquo;s SQLHelper class, passing in a stored procedure (and parameters), and our entities use the reader to load all their properties. Pretty straight forward stuff.
The problemis, on the live system, every few months a sproc will stop returning results, for no apparent reason.</description></item><item><title>Winforms Design Time support: exposing sub designers</title><link>https://andydote.co.uk/2012/10/29/winforms-design-time-support-exposing-sub-designers/</link><pubDate>Mon, 29 Oct 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/10/29/winforms-design-time-support-exposing-sub-designers/</guid><description>When writing a UserControl, it is often desired to expose one or more of the sub-controls design-time support to the user of your control. It is reasonably straight forward to do, and here is a rundown of how:
We start off with our UserControl, in this case the imaginatively named TestControl:
The code behind looks like this:
[Designer(typeof(TestControlDesigner))] public partial class TestControl : UserControl { public TestControl() { InitializeComponent(); } [DesignerSerializationVisibility(DesignerSerializationVisibility.</description></item><item><title>Designing the EventDistributor</title><link>https://andydote.co.uk/2012/04/23/designing-the-eventdistributor/</link><pubDate>Mon, 23 Apr 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/04/23/designing-the-eventdistributor/</guid><description>When it comes to developing a new class, I don&amp;rsquo;t tend to use TDD (Test Driven Development), I favour something I have named TAD - Test Aided Development. In other words, while I am for Unit Testing in general, designing something via writing tests sometimes feels too clunky and slow. I always write classes and methods with testing very much in mind, but I do not generally write the tests until later on in the process.</description></item><item><title>Model View Presenters: Composite Views</title><link>https://andydote.co.uk/2012/03/29/model-view-presenters-composite-views/</link><pubDate>Thu, 29 Mar 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/03/29/model-view-presenters-composite-views/</guid><description>Table of Contents: Introduction Presenter to View Communication View to Presenter Communication Composite Views Presenter / Application communication &amp;hellip; When working with MVP, it won&amp;rsquo;t be long before you come across the need for multiple views on one form. There are several ways to achive this, and which you choose is really down to how you intend to (re)use your views.
The first method for dealing with the sub views is to expose them as a property of your main view, and set them up in the main view&amp;rsquo;s presenter:</description></item><item><title>Model View Presenters: View to Presenter Communication</title><link>https://andydote.co.uk/2012/01/31/model-view-presenters-view-to-presenter-communication/</link><pubDate>Tue, 31 Jan 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/01/31/model-view-presenters-view-to-presenter-communication/</guid><description>Table of Contents: Introduction Presenter to View Communication View to Presenter Communication Composite Views Presenter / Application communication &amp;hellip; Communicating from the View to the Presenter is a reasonably straight forward affair. To signal something happening, we use an Event, but one with no parameters. We pass no parameters, as we are not going to be using them anyway, so what is the point is raising an event every time with OkayClicked(this, EventArgs.</description></item><item><title>Model View Presenters: Introduction</title><link>https://andydote.co.uk/2012/01/26/model-view-presenter-introduction/</link><pubDate>Thu, 26 Jan 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/01/26/model-view-presenter-introduction/</guid><description>Table of Contents Introduction Presenter to View Communication View to Presenter Communication Composite Views Presenter / Application communication &amp;hellip; What is MVP? I first came across MVP in Jeremy Miller&amp;rsquo;s Build Your Own Cab series, and have been using and improving how I work with this style ever since. Model View Presenters tend to come in one of two forms: Passive View, and Supervising Controller. I am a fan of the Passive View variety, primarily for the testing aspect, but also as I find it provides me with the best level of separation.</description></item><item><title>Model View Presenters: Presenter to View Communication</title><link>https://andydote.co.uk/2012/01/26/model-view-presenters-presenter-to-view-communication/</link><pubDate>Thu, 26 Jan 2012 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2012/01/26/model-view-presenters-presenter-to-view-communication/</guid><description>Table of Contents: Introduction Presenter to View Communication View to Presenter Communication Composite Views Presenter / Application communication &amp;hellip; Presenter to View Communication There are two styles utilised for populating the View with data from the Presenter and Model that I have used. The only difference between them is how tightly coupled you mind your View being to the Model. For the example of this, we will have the following as our Model:</description></item><item><title>Working with XmlTextWriter</title><link>https://andydote.co.uk/2011/10/25/working-with-xmltextwriter/</link><pubDate>Tue, 25 Oct 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/10/25/working-with-xmltextwriter/</guid><description>I was working on some code today that needs a lot of data writing into an XML document. The documents structure is not repetitive - it is loads of one time data, so templating the document is possible, but not the best route to go.
To that end, it uses an XmlTextWriter. The problem I have with it is the way you must write sub-elements. If you just need a single value wrapped in a tag, you are catered for already:</description></item><item><title>Noticing Changes</title><link>https://andydote.co.uk/2011/10/22/noticing-changes/</link><pubDate>Sat, 22 Oct 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/10/22/noticing-changes/</guid><description>I work on a piece of software that has been around for about 6 years now, which looks something like this:
The textboxes are validating that their contents, some as decimal, and some as integer. All the textboxes consider no-value to be invalid.
I made a slight change to the control, which was to add a new row. Since adding that row, many users have sent in requests to have the validation changed on the textboxes, so that no-value is considered to be zero.</description></item><item><title>C# and Vb.Net Differences</title><link>https://andydote.co.uk/2011/09/14/c-and-vbnet-differences/</link><pubDate>Wed, 14 Sep 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/09/14/c-and-vbnet-differences/</guid><description>So I have been doing some work that involves C# and VB libraries and apps using each other, and have noticed a lot of subtle differences between the two languages.
Declaration of types inside an interface: Public Interface ITesting ReadOnly Property Test() As TestData Class TestData Public Sub New() StringProperty = &amp;#34;testing&amp;#34; IntProperty = 1234 End Sub Public Property StringProperty() As String Public Property IntProperty() As Integer End Class End Interface However in C#, you cannot declare types inside an interface, however it is quite happy to consume one create in a VB project:</description></item><item><title>c# Enum casting</title><link>https://andydote.co.uk/2011/08/09/c-enum-casting/</link><pubDate>Tue, 09 Aug 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/08/09/c-enum-casting/</guid><description>I am all for strong typing, and explicit casts, but some things in C# do seem to be a bit over-wordy. For instance, I would quite often have code that looks like the following in VB.Net:
Public Enum Columns Name Value Action End Enum Private Sub InitialiseGrid(ByVal grid as SourceGrid.Grid) grid.ColumnCount = [Enum].GetValues(GetType(Columns)).Count grid.Columns(Columns.Name).AutoSizeMode = SourceGrid.AutoSizeMode.EnableAutoSizeView grid.Columns(Columns.Value).AutoSizeMode = SourceGrid.AutoSizeMode.EnableAutoSizeView | SourceGrid.AutoSizeMode.EnableStretch grid.Columns(Columns.Action).AutoSizeMode = SourceGrid.AutoSizeMode.None grid.Columns(Columns.Action).Width = 30 &amp;#39;etc... End Sub The problem arrives when you try to write the same in C#, specifically the part when accessing the Columns collection using the enum:</description></item><item><title>Differences between Properties and Auto Properties</title><link>https://andydote.co.uk/2011/07/11/differences-between-properties-and-auto-properties/</link><pubDate>Mon, 11 Jul 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/07/11/differences-between-properties-and-auto-properties/</guid><description>While writing some of the specs for ViewWeaver, I noticed that one was failing:
When passed a type with one write only property
it should return no mappings When I stepped through the code, it was indeed not filtering out the write only property. This is the code used to find all readable properties:
var allProperties = typeof(T).GetProperties(BindingFlags.Instance | BindingFlags.Public); var readableProperties = allProperties.Where(p =&amp;gt; p.CanRead &amp;amp;&amp;amp; !p.GetIndexParameters().Any()); For some reason CanRead was returning true, then I noticed how I had defined my class under test:</description></item><item><title>(Miss)Use of Narrowing-Implicit Operators</title><link>https://andydote.co.uk/2011/03/17/missuse-of-narrowing-implicit-operators/</link><pubDate>Thu, 17 Mar 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/03/17/missuse-of-narrowing-implicit-operators/</guid><description>I have covered a use of Narrowing/Implicit Operators before, but I was thinking the other day about use of Fluent Interfaces, and if it was possible to have one on a cache/repository type class, that would allow you to chain options together, but stop at any point and have the result.
I gave it a go, and came up with this:
public class Person { public string Name { get; set; } public int Age { get; set; } public Person(string name, int age) { this.</description></item><item><title>Expression Rules, Version 2</title><link>https://andydote.co.uk/2011/02/09/expression-rules-version-2/</link><pubDate>Wed, 09 Feb 2011 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2011/02/09/expression-rules-version-2/</guid><description>Recently I have written a rules engine for a very large menu system in an application I work on. Many of the rules apply many items, so I didn&amp;rsquo;t wish to have to express the same rule many times. To avoid this, the rule engine DSL was born:
Concerns.When(item =&amp;gt; /* rule of some sort */) .AppliesToAll() .Except(MenuItems.ToggleHidden, MenuItems.Refresh) And rules are rolled together, so a specific menu item must have all of its rules evaluating to true to be displayed.</description></item><item><title>Adding MSpec to your Git Bash</title><link>https://andydote.co.uk/2010/11/13/adding-mspec-to-your-git-bash/</link><pubDate>Sat, 13 Nov 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/11/13/adding-mspec-to-your-git-bash/</guid><description>My workflow involves Visual Studio, Notepad++ and Git Bash. I don&amp;rsquo;t use much Visual Studio integration, and prefer to run most things from the command line.
Now when it comes to testing projects, my tool of choice is MSpec (Machine.Specifications), which I decided would be nice if I could run from my Git Bash.
$ mspec bin/project.specs.dll To do this, you need to write a Shell Script with the following contents:</description></item><item><title>Databinding to a DataGridView - The order of columns</title><link>https://andydote.co.uk/2010/10/20/databinding-to-a-datagridview-the-order-of-columns/</link><pubDate>Wed, 20 Oct 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/10/20/databinding-to-a-datagridview-the-order-of-columns/</guid><description>A while ago I was writing a small history grid in one of our applications at work. It has a single HistoryItem object, which is fairly straightforward, something like this:
Class HistoryItem { public int ID { get{ return _id; } } public DateTime CreateDate { get { return _createDate; } } public String Creator { get { return _creatorName; } } public String Note { get { return _note; } } } This was populated into a List&amp;lt;HistoryItem&amp;gt; and bound to the DataGridView directly:</description></item><item><title>Actually, I'll mutate if you don't mind</title><link>https://andydote.co.uk/2010/09/10/actually-i-ll-mutate-if-you-don-t-mind/</link><pubDate>Fri, 10 Sep 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/09/10/actually-i-ll-mutate-if-you-don-t-mind/</guid><description>After I had changed all my extension methods to be functions and return a new object rather than mutating the self parameter, I changed them all back to be refs.
Why? Well mainly because the library I am writing is in VB, and these methods are internal. VB supports ByRef parameters as the first param in an extension method, so no problems there. The only reason I was changing them so that they were C# compatible was so that I could test them with MSpec in C#.</description></item><item><title>To mutate or not to mutate</title><link>https://andydote.co.uk/2010/09/08/to-mutate-or-not-to-mutate/</link><pubDate>Wed, 08 Sep 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/09/08/to-mutate-or-not-to-mutate/</guid><description>I have been working on a project recently that involves a lot of work with Flags Enums. To aid with this I created a set of Extension Methods:
Add(Of T as Structure)(self as T, value as Int) as T Add(Of T as Structure)(self as T, values() as Int) as T Remove(Of T as Structure)(self as T, value as Int) as T Remove(Of T as Structure)(self as T, values() as Int) as T Has(Of T as Structure)(self as T, value as Int) as Boolean HasAll(Of T as Structure)(self as T, values() as Int) as Boolean HasAny(Of T as Structure)(self as T, values() as Int) as Boolean Now the last 3 methods I am happy with - they are self explanatory and do what&amp;rsquo;s expected.</description></item><item><title>Using Visual Studio's Regex Find and Replace</title><link>https://andydote.co.uk/2010/08/31/using-visual-studio-s-regex-find-and-replace/</link><pubDate>Tue, 31 Aug 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/08/31/using-visual-studio-s-regex-find-and-replace/</guid><description>The Visual Studio Find and Replace dialog is often overlooked, and when parts of it are looked at (Regex searching) it often gets a bad rep. Sure it doesn&amp;rsquo;t implement all of the Regex syntax (non greedy search springs to mind), but that&amp;rsquo;s not to say it isn&amp;rsquo;t useful.
For instance, I was working on some code that involved a Model View Presenter type style, but used Subroutines (void methods) rather than WriteOnly properties for brevity (in C# you can do a Set only property in 1 line, VB it takes 5).</description></item><item><title>Multilining If statements conditions should be banned. now.</title><link>https://andydote.co.uk/2010/03/24/multilining-if-statements-conditions-should-be-banned-now/</link><pubDate>Wed, 24 Mar 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/03/24/multilining-if-statements-conditions-should-be-banned-now/</guid><description>Multilining if statement conditions is bad. I was modifying some code and came across this:
If String.IsNullOrEmpty(_selectedGUID) OrElse _ _selectedGUID = FeeAgreement.GetDefaultContractAgreementGuid OrElse _ _selectedGUID = FeeAgreement.DefaultPermAgreementGuid Then fgFeeAgreements.SetCellCheck(rowAdded, 0, CheckEnum.Checked) _selectedTitle = ag.Title _lastIndexRowSelected = rowAdded End If Which at a glance looks like this:
Single Line If Variable Assignment Variable Assignment
One person suggested that if someone had to do multiline the condition they could at least indent it.</description></item><item><title>Converting from NUnit to MSTest</title><link>https://andydote.co.uk/2010/01/12/converting-from-nunit-to-mstest/</link><pubDate>Tue, 12 Jan 2010 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2010/01/12/converting-from-nunit-to-mstest/</guid><description>While this is not something I personally would want to do, we (for whatever reason&amp;hellip;) are to use MSTest at work (I think it is due to the whole &amp;ldquo;Its Microsoft, so it&amp;rsquo;s supported&amp;rdquo; argument).
Now as no one else on the team does any kind of unit testing (serious), the only test projects we have are written by me, on the quiet before being told if I wanted to unit test then use MSTest.</description></item><item><title>Thanks Google for solving my problem!</title><link>https://andydote.co.uk/2009/12/16/thanks-google-for-solving-my-problem/</link><pubDate>Wed, 16 Dec 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/12/16/thanks-google-for-solving-my-problem/</guid><description>Following on from yesterday&amp;rsquo;s post about separation on concerns and where to put some undefined logic for a multi state checkbox, I did a fair amount of research.
I must say the Quince website is a good repository of UI Design Patterns, as is Welie. I couldn&amp;rsquo;t find anything like what I was after, which I guess means I shouldn&amp;rsquo;t be doing it this way?
After a while a brainwave struck me: &amp;ldquo;Gmail lets you select things, how does it do it?</description></item><item><title>Functionality and Seperation of Concerns</title><link>https://andydote.co.uk/2009/12/15/functionality-and-seperation-of-concerns/</link><pubDate>Tue, 15 Dec 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/12/15/functionality-and-seperation-of-concerns/</guid><description>When I am writing a winform in an MVP style, I often wonder how far to go with the separation. Say I have the following situation:
A small form which should display a list of messages, and allow the user to select which ones they want processed. It processes each message in turn. If a message has more than one attachment, a dialog is shown to ask the user to select which attachment should be used for that message.</description></item><item><title>Software Fuzzying maybe?</title><link>https://andydote.co.uk/2009/10/07/software-fuzzying-maybe/</link><pubDate>Wed, 07 Oct 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/10/07/software-fuzzying-maybe/</guid><description>This morning I read this post by Alfred Thompson about whether we are Software Engineers or something else. I can&amp;rsquo;t help but agree with him, as I don&amp;rsquo;t feel we are engineers (yet), our discipline is a little fuzzy to be classified as engineering I think.
However I think we are not alone in this boat. Not all engineering disciplines are quite so well cut. My father is an Engineering Pattern Maker.</description></item><item><title>Region Hate</title><link>https://andydote.co.uk/2009/10/06/region-hate/</link><pubDate>Tue, 06 Oct 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/10/06/region-hate/</guid><description>There seems to be a lot of negativity towards the #Region in .net at the moment, with many people hating them and calling all usages of them &amp;lsquo;retarded&amp;rsquo;.
I can see their point, especially when you see the odd class with regions like this:
Class Foo { #Private Members #Protected Members #Friend Members #Public Members #Private Constructors #Protected Constructors #Friend Constructors #Public Constructors #Private Methods #Protected Methods #Friend Methods #Public Methods } Clearly the person who wrote this was ill at the time (I hope&amp;hellip;), and besides, where would Protected Friends go?</description></item><item><title>Fluency at a cost?</title><link>https://andydote.co.uk/2009/07/29/fluency-at-a-cost/</link><pubDate>Wed, 29 Jul 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/07/29/fluency-at-a-cost/</guid><description>I like fluent interfaces. I find them easy to read, and nice to program with. However the more I write them the more I notice there is a cost associated with them. It&amp;rsquo;s not much of a cost, but it is there none the less. To demonstrate say we have a class called Animator. It has the following properties and methods on it:
+ Control + Distance + DistanceType + AnimationType + Direction + Time + Algorithm - Animate() Now while you could just set all the properties and then call Animate(), a Fluent Interface makes thing nicer:</description></item><item><title>Key Bindings</title><link>https://andydote.co.uk/2009/07/17/key-bindings/</link><pubDate>Fri, 17 Jul 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/07/17/key-bindings/</guid><description>When I was at college studying Electronics and Computer Engineering, we used a piece of software called Proteus. This software took a long time to get used to due to its interesting key bindings and mouse usage.
To select a track in Ares (PCB Layout package) you Right Click on it. Hmm not too standard, but okay, I can live with that. Now what happens if you were to right click on that track again?</description></item><item><title>CI: Thoughts on CC.Net and Hudson</title><link>https://andydote.co.uk/2009/07/14/ci-thoughts-on-ccnet-and-hudson/</link><pubDate>Tue, 14 Jul 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/07/14/ci-thoughts-on-ccnet-and-hudson/</guid><description>I have been a fan of CI (Continuous Integration) for a long time now, and ever since I started with CI I have been using CruiseControl.Net. CCNet is incredibly powerful; you can make to do practically anything, and writing plugins for it is a breeze.
However, I do find that the config files get rather messy. I have tried many things and the current best solution seems to be to have one &amp;lsquo;master&amp;rsquo; config file with a set of includes to other files.</description></item><item><title>Overuse of the Var keyword</title><link>https://andydote.co.uk/2009/06/29/overuse-of-the-var-keyword/</link><pubDate>Mon, 29 Jun 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/06/29/overuse-of-the-var-keyword/</guid><description>When I first got hold of VS2008, and had a play with the new version of C# I loved the Var keyword. To me the most amazing thing was no more declarations like this:
System.Text.RegularExpressions.Regex rx = new System.Text.RegularExpressions.Regex(); Instead I could write the following:
var rx = new System.Text.RegularExpressions.Regex(); Making it akin to VB developers being able to write:
Dim rx As New System.Text.RegularExpressions.Regex() (I have had however to cope with a coding standard that explicitly forbid this declaration in VB&amp;hellip;Backwards or what?</description></item><item><title>Coming From Something as opposed to Going To Something</title><link>https://andydote.co.uk/2009/06/19/coming-from-something-as-opposed-to-going-to-something/</link><pubDate>Fri, 19 Jun 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/06/19/coming-from-something-as-opposed-to-going-to-something/</guid><description>Over the last week I have noticed myself preferring methods being called IntegerFromString rather than StringToInteger. Is sometimes takes me a little longer to read (only a few milliseconds, mind) but I think I am getting more used to it, and I do think it enhances readability.
The main point for readability comes from the fact that I work a lot (in my spare time when coding) on graphics processing in GDI.</description></item><item><title>Fluent Validation</title><link>https://andydote.co.uk/2009/06/12/fluent-validation/</link><pubDate>Fri, 12 Jun 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/06/12/fluent-validation/</guid><description>A few days a go i was going through my bookmarks, and came accross this post on the GetPaint.Net blog about using a fluent interface for parameter validation.
After reading the article, I tried the code out at home, and was very impressed. Not only does it read well, but also does not create any objects untill a piece of validation fails. Very nice.
However i wanted to use this at work, and this presented me with a problem.</description></item><item><title>The Reading List</title><link>https://andydote.co.uk/2009/06/05/the-reading-list/</link><pubDate>Fri, 05 Jun 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/06/05/the-reading-list/</guid><description>I have been meaning to write my own version of Jeff Atwood&amp;rsquo;s Reading List for a while now, and have finally managed to find some time to write about the books I have read.
Code Complete 2 I found this book quite tough reading. It wasn&amp;rsquo;t that it had nothing of use in it - far from it, it just was fairly heavy going. There are many things that can be learnt from this to help with the whole process of development, from how to name variables and functions to what the useful methods of development planning are.</description></item><item><title>Converting Code</title><link>https://andydote.co.uk/2009/06/01/converting-code/</link><pubDate>Mon, 01 Jun 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/06/01/converting-code/</guid><description>Quite often (well ok, not that often) I am asked why I convert most C# code I find into VB.Net. The main reason is I find that it helps me to understand the code.
To put it into perspective, my Test Projects directory contains around 50 projects with about a 50/50 split of C# projects and VB.Net projects.
When I come across a sample online (often in a blog) it is either small enough to be understood straight away, or something much larger that needs a lot of thinking and looking at (look at Jeremy Miller&amp;rsquo;s Build Your Own CAB Series) to fully understand.</description></item><item><title>Microcontrollers for MenuItems</title><link>https://andydote.co.uk/2009/05/29/microcontrollers-for-menuitems/</link><pubDate>Fri, 29 May 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/05/29/microcontrollers-for-menuitems/</guid><description>I have been working my way through Jeremy Miller&amp;rsquo;s excellent Build Your Own CAB Series (which would be even better if he felt like finishing!) and was very interested by the article on controlling menus with Microcontrollers.
After reading it and writing a version of it myself, I came to the conclusion that some parts of it seem to be wrong. All of the permissioning is done based on the menu items which fire ICommands, and several menu items could use the same ICommand.</description></item><item><title>Generics to the rescue! Again!</title><link>https://andydote.co.uk/2009/05/22/generics-to-the-rescue-again/</link><pubDate>Fri, 22 May 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/05/22/generics-to-the-rescue-again/</guid><description>I was writing a component at work that has many events that all need to be thread safe, and was getting annoyed at the amount of duplicate code I was writing:
Public Event FilterStart(ByVal sender As Object, ByVal e As EventArgs) &amp;#39;... Private Delegate Sub OnFilterCompleteDelegate(ByVal sender As Object, ByVal e As FilterCompleteEventArgs) &amp;#39;... Private Sub OnFilterComplete(ByVal sender As Object, ByVal e As DataAccess.LoadEventArgs) If _parent.InvokeRequired Then _parent.Invoke(new OnFilterCompleteDelegate(AddressOf OnFilterComplete), new Object() {sender, e}) Else RaiseEvent FullResultsStart(sender, e) End If End Sub &amp;#39;.</description></item><item><title>Using Laziness</title><link>https://andydote.co.uk/2009/05/19/using-laziness/</link><pubDate>Tue, 19 May 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/05/19/using-laziness/</guid><description>As I do a lot of forms development, I end up writing something like this a lot:
Try pnlSomething.SuspendLayout() &amp;#39;... Finally pnlSomething.ResumeLayout() End Try Now as I am lazy, I thought I could make a class to do this for me:
Public Class Layout Implements IDisposable Private _control As Control Public Sub New(ByVal control As Control) _control = control _control.SuspendLayout() End Sub Public Sub Dispose() Implements IDisposable.Dispose _control.ResumeLayout() _control = Nothing End Sub End Class It is used like this:</description></item><item><title>Finally, I have used a Model View Controller!</title><link>https://andydote.co.uk/2009/05/18/finally-i-have-used-a-model-view-controller/</link><pubDate>Mon, 18 May 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/05/18/finally-i-have-used-a-model-view-controller/</guid><description>Today I actually managed to use a Model View Controller in an application. I have been looking for an opportunity to use one fore a while, and have been reading a lot about them (Jeremy Miller&amp;rsquo;s Build Your Own CAB Series has been a very good guide).
The type of MVC I like most (so far) is the Passive View type, where the View does almost nothing, and has no link to the Model:</description></item><item><title>SQL Like statement</title><link>https://andydote.co.uk/2009/05/15/sql-like-statement/</link><pubDate>Fri, 15 May 2009 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2009/05/15/sql-like-statement/</guid><description>Today I learnt a few new (well to me) SQL commands. The Like statement can do some basic regex type things. It supports character specifiers like this:
Column Like &amp;#39;%[a-z]Test[a-z]%&amp;#39; This will find the word test as long as there is a letter at either end of the word in a block of text. You can also say Not a letter like so:
Column Like &amp;#39;%[^a-z]Test[^a-z]%&amp;#39; This should find any words Test that do not have letters before or after them.</description></item><item><title>Conflicting Unrelated Options: Alps Trackpad vs Microsoft Mouse</title><link>https://andydote.co.uk/2008/04/17/conflicting-unrelated-options-alps-trackpad-vs-microsoft-mouse/</link><pubDate>Thu, 17 Apr 2008 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2008/04/17/conflicting-unrelated-options-alps-trackpad-vs-microsoft-mouse/</guid><description>Usability. It&amp;rsquo;s one of those things that everyone wants, you know, stuff that &amp;lsquo;just works&amp;rsquo;. It&amp;rsquo;s nice when companies go out of their way to make things &amp;lsquo;just work&amp;rsquo;. It&amp;rsquo;s a shame Sony (and others, but I have a Sony, so it&amp;rsquo;s their fault for this exercise) decided to make things harder for me.
Allow me to explain. I have a Sony Vaio (VGN-FE21M), which has an Alps touch pad, which like all touch pad has tapping enabled by default, and that I switch off straight away.</description></item><item><title>Creating Non resizable controls</title><link>https://andydote.co.uk/2008/04/13/creating-non-resizable-controls/</link><pubDate>Sun, 13 Apr 2008 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2008/04/13/creating-non-resizable-controls/</guid><description>A control I was recently developing required being non-resizable when on the form. When the application is running, this would be easy enough, just set its AutoSize property to False, and don&amp;rsquo;t dock the control.
However, this leaves the problem of resizing in the designer. You could override the resize event of the control, but for reasons outlined earlier, such as flickering, I decided against this.
Somewhere on the internet (where else&amp;hellip;?</description></item><item><title>Vaio Event Service and Vista</title><link>https://andydote.co.uk/2008/04/08/vaio-event-service-and-vista/</link><pubDate>Tue, 08 Apr 2008 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2008/04/08/vaio-event-service-and-vista/</guid><description>When I first installed vista onto my laptop (a Sony Vaio VGN-FE21M) I also installed all the Sony stuff that I needed for it, mainly the VES (Vaio Event Service) and Control Panel, although there is a lot of other junk that they give you to install.
In fact the first thing I did when I had the laptop new was to wipe MCE 2005 off it, and install XP Pro.</description></item><item><title>Flow Layout Panel and Scroll Wheel Problem</title><link>https://andydote.co.uk/2008/03/29/flow-layout-panel-and-scroll-wheel-problem/</link><pubDate>Sat, 29 Mar 2008 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2008/03/29/flow-layout-panel-and-scroll-wheel-problem/</guid><description>I came across a problem while writing an application for my parents today. If you have a FlowLayoutPanel on your form, and have many items in it, causing it to overflow and require scroll bars, you are unable to scroll the control&amp;rsquo;s content using the mouse wheel.
This is somewhat trying in today&amp;rsquo;s applications, as nearly everyone has a mouse with a scroll wheel, or a track pad on their laptop with a scroll area.</description></item><item><title>VB.NET &amp;amp; C# Fixed height User Controls</title><link>https://andydote.co.uk/2008/03/29/vbnet-c-fixed-height-user-controls/</link><pubDate>Sat, 29 Mar 2008 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2008/03/29/vbnet-c-fixed-height-user-controls/</guid><description>Another problem I came across recently was fixed height user controls. Someone at work had created a fixed height user control, by putting the following code in the paint event:
Me.Width = 20 Now while for the majority of cases this works, it doesn&amp;rsquo;t if you dock the control to the left or right of the form, as each time the Layout Engine tries to stick the top of the control to the top of the parent and the bottom of the control to the bottom of the parent, it fires the Paint() event.</description></item></channel></rss>