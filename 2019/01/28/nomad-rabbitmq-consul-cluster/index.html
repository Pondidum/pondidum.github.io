<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RabbitMQ clustering with Consul in Nomad | Andy Dote</title><meta name=keywords content="infrastructure,vagrant,nomad,consul,rabbitmq"><meta name=description content="Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.
RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i."><meta name=author content><link rel=canonical href=https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/><link crossorigin=anonymous href=/assets/css/stylesheet.min.4ac25d88867f6882d86478d9b478a3d3efa1ed9e18f0bc5e432812301516cb28.css integrity="sha256-SsJdiIZ/aILYZHjZtHij0++h7Z4Y8LxeQygSMBUWyyg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/mermaid.min.725f44bd345b0a2a4043ca952b0863edd789e913cf0813a12bbdfe986fe87079.js integrity="sha256-cl9EvTRbCipAQ8qVKwhj7deJ6RPPCBOhK73+mG/ocHk="></script>
<script defer crossorigin=anonymous src=/js/tabs.min.2d019e9ee3574770ad4ecfd4f5f794739892195cb82a4e6383252b9074ab520c.js integrity="sha256-LQGenuNXR3CtTs/U9feUc5iSGVy4Kk5jgyUrkHSrUgw="></script>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andydote.co.uk/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andydote.co.uk/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andydote.co.uk/favicon-32x32.png><link rel=apple-touch-icon href=https://andydote.co.uk/apple-touch-icon.png><link rel=mask-icon href=https://andydote.co.uk/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="RabbitMQ clustering with Consul in Nomad"><meta property="og:description" content="Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.
RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i."><meta property="og:type" content="article"><meta property="og:url" content="https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-01-28T00:00:00+00:00"><meta property="article:modified_time" content="2019-01-28T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RabbitMQ clustering with Consul in Nomad"><meta name=twitter:description content="Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.
RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andydote.co.uk/post/"},{"@type":"ListItem","position":3,"name":"RabbitMQ clustering with Consul in Nomad","item":"https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RabbitMQ clustering with Consul in Nomad","name":"RabbitMQ clustering with Consul in Nomad","description":"Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.\nRabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i.","keywords":["infrastructure","vagrant","nomad","consul","rabbitmq"],"articleBody":"Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.\nRabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i.e. one picked by Nomad.)\nI ran into a few problems trying to get this working, but a lot of searching and some help from the RabbitMQ mailing list (thanks Luke!) got me through all the issues, so hopefully, this will be easier next time and for other people too.\nIt is also worth noting that this is only going to be covering how to make a cluster work, not how to make it secure (setting up TLS etc.) for production usage. There is a lot of documentation on the RabbitMQ website for further reading on this!\nThe full repository with all of the demo code is available on my Github.\nNomad Cluster As this post is mostly about running RabbitMQ on Nomad, and not setting up Nomad, I’ll give the basics here - the full scripts are in the repository, and linked below too.\nVagrant is used to create us a three node cluster. As I use Hyper-V for VMs, I can’t set static IPs from the Vagrant file, so I have used another Vagrant feature: triggers.\nTriggers let us specify scripts to run after Vagrant actions, so in this case, we run a script after machine1 comes up which writes out it’s IP to the /vagrant share. The other machines can then read this same file to join the cluster:\nVagrant.configure(2) do |config| config.vm.box = \"bento/ubuntu-16.04\" config.vm.provision \"shell\", path: \"./provision.sh\", privileged: false config.vm.define \"n1\" do |n1| n1.vm.provision \"shell\", path: \"./server.sh\", privileged: false n1.trigger.after :up do |trigger| trigger.run_remote = { inline: \"ip route get 1 | awk '{print $NF;exit}' \u003e /vagrant/server_ip\" } end end config.vm.define \"n2\" do |n2| n2.vm.hostname = \"nomad2\" n2.vm.provision \"shell\", path: \"./client.sh\", privileged: false end end The provision.sh script downloads and installs both Consul and Nomad, and then the respective server.sh and client.sh scripts set up both services in the right ways. The server machine also acts as a Nomad client, so that we don’t need 4 VMs running.\nVagrantFile, provision.sh, server.sh, client.sh\nAll that remains to be done is vagrant up…and wait. But while we’re waiting for the machines to provision, let’s have a look at RabbitMQ clustering, and how we’ll define the job in Nomad.\nRabbitMQ Cluster A few things to note about clustering RabbitMQ:\nAll nodes must be listening on the same port for clustering (4369 by default) The ERL_EPMD_PORT variable doesn’t work on rabbitmq \u003c 3.7.9 The latest Docker image for rabbitmq is 3.7.8 The rabbit node names must be DNS resolvable The RABBITMQ_ERLANG_COOKIE must have the same value The rabbitmq_peer_discovery_consul plugin we will use is shipped with RabbitMQ by default but is disabled. The easiest way to get everything up and running is to create your own docker container, with the plugin enabled, and a small configuration file to set a few options:\nFROM rabbitmq:management-alpine COPY rabbitmq.conf /etc/rabbitmq RUN rabbitmq-plugins enable --offline rabbitmq_peer_discovery_consul The rabbitmq.conf only needs a few lines:\ncluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul cluster_formation.consul.svc_addr_auto = true docker build -t rabbitmq:consul . An image built from this is also available on docker hub.\nOnce we have a custom container built, it’s a good idea to test that it actually works, before we start trying to get Nomad to run it. We’ll do this by creating a network in Docker so that all the containers can talk directly to each other on their pre-defined ports. Don’t forget to change CONSUL_HOST to your machine’s IP address!\ndocker network create rabbit docker run -d --rm --name consul -p 8500:8500 consul docker run -d --rm --name rabbit1 -h rabbit1 --network rabbit -p 30001:15672 -e RABBITMQ_ERLANG_COOKIE='rabbit' -e 'RABBITMQ_DEFAULT_USER=test' -e 'RABBITMQ_DEFAULT_PASS=test' -e CONSUL_HOST='10.0.75.1' rabbitmq:consul docker run -d --rm --name rabbit2 -h rabbit2 --network rabbit -p 30002:15672 -e RABBITMQ_ERLANG_COOKIE='rabbit' -e 'RABBITMQ_DEFAULT_USER=test' -e 'RABBITMQ_DEFAULT_PASS=test' -e CONSUL_HOST='10.0.75.1' rabbitmq:consul docker run -d --rm --name rabbit3 -h rabbit3 --network rabbit -p 30003:15672 -e RABBITMQ_ERLANG_COOKIE='rabbit' -e 'RABBITMQ_DEFAULT_USER=test' -e 'RABBITMQ_DEFAULT_PASS=test' -e CONSUL_HOST='10.0.75.1' rabbitmq:consul You can now visit http://localhost:30001 (or 30002 or 30003) and see that we have a successful cluster running. Once you’re happy with it, you can kill it all off (as we started the containers with the --rm flag, Docker will delete them for us when they stop):\ndocker stop rabbit1 rabbit2 rabbit3 consul docker network rm rabbit Nomad Rabbit Job Now that we know our container clusters successfully, we can create a Job definition to do the same thing in Nomad. Nomad jobs are defined in HCL, a Json-like configuration language.\nThe jobs require a name, which datacentre it should run in, and what kind of job type it is. In this case, our job is called rabbit (imaginative I know), we’ll run it in dc1 (the default value Nomad starts with), and we’ll make this job be a service, as opposed to a batch or system job:\njob \"rabbit\" { datacenters = [\"dc1\"] type = \"service\" group \"cluster\" { # tasks ... } } The group is used to hold a collection of tasks, and when allocating a job, Nomad will make sure that all tasks belonging to an instance of a group are on the same node.\nSo for example, if you had a 2 node Nomad cluster, and 3 instances of a group which contained 3 tasks (e.g. API, varnish, and nginx containers), Nomad might distribute the groups like so:\nWithin the group, we can specify the number of instances we want with the count property, and we also specify that for both updates and migrations, only one group can be changed at a time. This means that if you decide to upgrade the container used by the job, Nomad won’t stop all instances at once, destroying your service’s availability!\nWe also specify that we want to use the health checks (defined later on) rather than the state of the task itself to determine what is healthy, and how long the task must be healthy for before we decide it’s actually healthy, and how long it has to achieve being healthy.\ngroup \"cluster\" { count = 3 update { max_parallel = 1 } migrate { max_parallel = 1 health_check = \"checks\" min_healthy_time = \"5s\" healthy_deadline = \"30s\" } } The task is our unit of work in Nomad. In this case, we are using the docker driver, but it also supports many other drivers including exec, rkt and lxc. We configure which image to use, and importantly that the hostname is the name from Nomad!\nThe port_map tells nomad which ports of the container we want to expose, and labels them. We can then refer to the ports by their labels in other parts of the configuration.\ntask \"rabbit\" { driver = \"docker\" config { image = \"pondidum/rabbitmq:consul\" hostname = \"${attr.unique.hostname}\" port_map { amqp = 5672 ui = 15672 epmd = 4369 clustering = } } } The env section is pretty self-explanatory; they are environment variables to pass to the container. As Consul is running on the Nomad host, we use the Nomad interpolation attribute to specify the IP of the current host, and we also set the RABBITMQ_ERLANG_COOKIE to a specific value. In a production environment, you should be setting this value to something unguessable, possibly using the Vault intergration in Nomad to fetch a token. We can also add other settings to pass to the container here, such as RABBITMQ_DEFAULT_USER and RABBITMQ_DEFAULT_PASS. As with the cookie generation, in a production-like environment, you’d probably want to use the Vault integration to pull the values for these variables.\nenv { RABBITMQ_ERLANG_COOKIE = \"rabbitmq\" CONSUL_HOST = \"${attr.unique.network.ip-address}\" } The resources section lets us constraints on things like CPU, Memory, IOPs, and Network. In our case, we are only specifying a set of ports to expose on the network, and that we want them to be bound to specific ports on the host:\nresources { network { port \"amqp\" { static = 5672 } port \"ui\" { static = 15672 } port \"epmd\" { static = 4369 } port \"clustering\" { static = 25672 } } } We could select different ports to bind the container ports to, or leave out the static pair entirely to have Nomad map the ports to random unused ports on the host.\nFinally, the service block integrates with service discovery (so, Consul), and allows us to register ports and health checks for our service. In the case of our RabbitMQ cluster, we already have service discovery integration via the RabbitMQ Consul plugin, so this registration is only used for the check feature, which is what will also be used by the migrate block to see if a task is healthy:\nservice { check { name = \"alive\" type = \"tcp\" port = \"ui\" interval = \"10s\" timeout = \"2s\" } } The check is using the ui port defined earlier to check if the UI is alive. We could also change the health check to use the amqp port instead, as that might be a better indication that the actual service can do useful things. We can define multiple checks, and are not limited to TCP; grpc, http, and script are also supported.\nThe entire job definition is below, and is also available in the repository.\njob \"rabbit\" { datacenters = [\"dc1\"] type = \"service\" group \"cluster\" { count = 3 update { max_parallel = 1 } migrate { max_parallel = 1 health_check = \"checks\" min_healthy_time = \"5s\" healthy_deadline = \"30s\" } task \"rabbit\" { driver = \"docker\" config { image = \"pondidum/rabbitmq:consul\" hostname = \"${attr.unique.hostname}\" port_map { amqp = 5672 ui = 15672 epmd = 4369 clustering = 25672 } } env { RABBITMQ_ERLANG_COOKIE = \"generate_a_guid_-_or_something_for_this\" RABBITMQ_DEFAULT_USER = \"test\" RABBITMQ_DEFAULT_PASS = \"test\" CONSUL_HOST = \"${attr.unique.network.ip-address}\" } resources { network { port \"amqp\" { static = 5672 } port \"ui\" { static = 15672 } port \"epmd\" { static = 4369 } port \"clustering\" { static = 25672 } } } service { name = \"rabbitmq\" port = \"ui\" check { name = \"alive\" type = \"tcp\" interval = \"10s\" timeout = \"2s\" } } } } } Running The Job First, make sure your console can talk to Nomad, which we can do by using the server_ip file again:\nexport NOMAD_ADDR=\"http://$(cat server_ip):4646\" Now it should be possible to run the job:\nnomad job run rabbit/rabbit.nomad After a few moments, we can visit any of the Nomad hosts, and log in to the RabbitMQ UI (http://SOME_SERVER_IP:15672) and you should be greeted with a running cluster with three nodes:\nHomework Kill a container on one of the nodes and see what happens (vagrant ssh n2 then docker stop ) Create an Application which you deploy to Nomad which uses service discovery to talk to RabbitMQ Create some more Nomad clients, and try making a bigger RabbitMQ cluster If you have any questions, feel free to comment below or send me a tweet.\n","wordCount":"1879","inLanguage":"en","datePublished":"2019-01-28T00:00:00Z","dateModified":"2019-01-28T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/"},"publisher":{"@type":"Organization","name":"Andy Dote","logo":{"@type":"ImageObject","url":"https://andydote.co.uk/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andydote.co.uk accesskey=h title="Andy Dote (Alt + H)">Andy Dote</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://andydote.co.uk/archive title=Archive><span>Archive</span></a></li><li><a href=https://andydote.co.uk/talks title=Talks><span>Talks</span></a></li><li><a href=https://andydote.co.uk/notes title=Notes><span>Notes</span></a></li><li><a href=https://andydote.co.uk/tags title=Tags><span>Tags</span></a></li><li><a href=https://andydote.co.uk/contact title=Contact><span>Contact</span></a></li><li><a href=https://andydote.co.uk/rss.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>RabbitMQ clustering with Consul in Nomad</h1><div class=post-meta><span title='2019-01-28 00:00:00 +0000 UTC'>January 28, 2019</span>&nbsp;·&nbsp;9 min</div></header><div class=post-content><p><strong>Update</strong> If you want a secure version of this cluster, see <a href=/2019/04/06/nomad-rabbitmq-secure/>Running a Secure RabbitMQ Cluster in Nomad</a>.</p><p>RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the <a href=https://www.rabbitmq.com/clustering.html#cluster-formation-options>auto clustering plugins</a>, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use <a href=https://www.nomadproject.io/>Nomad</a> for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i.e. one picked by Nomad.)</p><p>I ran into a few problems trying to get this working, but a lot of searching and some help from the <a href=https://groups.google.com/forum/#!forum/rabbitmq-users>RabbitMQ mailing list</a> (thanks Luke!) got me through all the issues, so hopefully, this will be easier next time and for other people too.</p><p>It is also worth noting that this is only going to be covering how to make a cluster work, not how to make it secure (setting up TLS etc.) for production usage. There is a lot of <a href=https://www.rabbitmq.com/production-checklist.html#security-considerations>documentation on the RabbitMQ website</a> for further reading on this!</p><p>The full repository with all of the <a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo>demo code is available on my Github</a>.</p><h2 id=nomad-cluster>Nomad Cluster<a hidden class=anchor aria-hidden=true href=#nomad-cluster>#</a></h2><p>As this post is mostly about running RabbitMQ on Nomad, and not setting up Nomad, I&rsquo;ll give the basics here - the full scripts are in the repository, and linked below too.</p><p>Vagrant is used to create us a three node cluster. As I use Hyper-V for VMs, I can&rsquo;t set static IPs from the Vagrant file, so I have used another Vagrant feature: triggers.</p><p>Triggers let us specify scripts to run after Vagrant actions, so in this case, we run a script after machine1 comes up which writes out it&rsquo;s IP to the <code>/vagrant</code> share. The other machines can then read this same file to join the cluster:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ruby data-lang=ruby><span style=display:flex><span><span style=color:#66d9ef>Vagrant</span><span style=color:#f92672>.</span>configure(<span style=color:#ae81ff>2</span>) <span style=color:#66d9ef>do</span> <span style=color:#f92672>|</span>config<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span>  config<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>box <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;bento/ubuntu-16.04&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  config<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>provision <span style=color:#e6db74>&#34;shell&#34;</span>, <span style=color:#e6db74>path</span>: <span style=color:#e6db74>&#34;./provision.sh&#34;</span>, <span style=color:#e6db74>privileged</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  config<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>define <span style=color:#e6db74>&#34;n1&#34;</span> <span style=color:#66d9ef>do</span> <span style=color:#f92672>|</span>n1<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span>    n1<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>provision <span style=color:#e6db74>&#34;shell&#34;</span>, <span style=color:#e6db74>path</span>: <span style=color:#e6db74>&#34;./server.sh&#34;</span>, <span style=color:#e6db74>privileged</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    n1<span style=color:#f92672>.</span>trigger<span style=color:#f92672>.</span>after <span style=color:#e6db74>:up</span> <span style=color:#66d9ef>do</span> <span style=color:#f92672>|</span>trigger<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span>      trigger<span style=color:#f92672>.</span>run_remote <span style=color:#f92672>=</span> { <span style=color:#e6db74>inline</span>: <span style=color:#e6db74>&#34;ip route get 1 | awk &#39;{print $NF;exit}&#39; &gt; /vagrant/server_ip&#34;</span> }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  config<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>define <span style=color:#e6db74>&#34;n2&#34;</span> <span style=color:#66d9ef>do</span> <span style=color:#f92672>|</span>n2<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span>    n2<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>hostname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;nomad2&#34;</span>
</span></span><span style=display:flex><span>    n2<span style=color:#f92672>.</span>vm<span style=color:#f92672>.</span>provision <span style=color:#e6db74>&#34;shell&#34;</span>, <span style=color:#e6db74>path</span>: <span style=color:#e6db74>&#34;./client.sh&#34;</span>, <span style=color:#e6db74>privileged</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>The <code>provision.sh</code> script downloads and installs both Consul and Nomad, and then the respective <code>server.sh</code> and <code>client.sh</code> scripts set up both services in the right ways. The server machine also acts as a Nomad client, so that we don&rsquo;t need 4 VMs running.</p><p><a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo/blob/master/vagrantfile>VagrantFile</a>, <a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo/blob/master/provision.sh>provision.sh</a>, <a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo/blob/master/server.sh>server.sh</a>, <a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo/blob/master/client.sh>client.sh</a></p><p>All that remains to be done is <code>vagrant up</code>&mldr;and wait. But while we&rsquo;re waiting for the machines to provision, let&rsquo;s have a look at RabbitMQ clustering, and how we&rsquo;ll define the job in Nomad.</p><h2 id=rabbitmq-cluster>RabbitMQ Cluster<a hidden class=anchor aria-hidden=true href=#rabbitmq-cluster>#</a></h2><p>A few things to note about clustering RabbitMQ:</p><ul><li>All nodes must be listening on the same port for clustering (<code>4369</code> by default)</li><li>The <code>ERL_EPMD_PORT</code> variable doesn&rsquo;t work on <code>rabbitmq &lt; 3.7.9</code></li><li>The latest Docker image for rabbitmq is <code>3.7.8</code></li><li>The rabbit node names must be DNS resolvable</li><li>The <code>RABBITMQ_ERLANG_COOKIE</code> must have the same value</li></ul><p>The <code>rabbitmq_peer_discovery_consul</code> plugin we will use is shipped with RabbitMQ by default but is disabled. The easiest way to get everything up and running is to create your own docker container, with the plugin enabled, and a small configuration file to set a few options:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-conf data-lang=conf><span style=display:flex><span>FROM rabbitmq<span style=color:#960050;background-color:#1e0010>:</span>management-alpine
</span></span><span style=display:flex><span>COPY rabbitmq.conf <span style=color:#960050;background-color:#1e0010>/</span>etc<span style=color:#960050;background-color:#1e0010>/</span>rabbitmq
</span></span><span style=display:flex><span>RUN rabbitmq-plugins enable --offline rabbitmq_peer_discovery_consul
</span></span></code></pre></div><p>The <code>rabbitmq.conf</code> only needs a few lines:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-conf data-lang=conf><span style=display:flex><span>cluster_formation.peer_discovery_backend <span style=color:#f92672>=</span> rabbit_peer_discovery_consul
</span></span><span style=display:flex><span>cluster_formation.consul.svc_addr_auto <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build -t rabbitmq:consul .
</span></span></code></pre></div><p>An image built from this is also available on <a href=https://hub.docker.com/r/pondidum/rabbitmq>docker hub</a>.</p><p>Once we have a custom container built, it&rsquo;s a good idea to test that it actually works, before we start trying to get Nomad to run it. We&rsquo;ll do this by creating a network in Docker so that all the containers can talk directly to each other on their pre-defined ports. Don&rsquo;t forget to change <code>CONSUL_HOST</code> to your machine&rsquo;s IP address!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker network create rabbit
</span></span><span style=display:flex><span>docker run -d --rm --name consul -p 8500:8500 consul
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docker run -d --rm --name rabbit1 -h rabbit1 --network rabbit -p 30001:15672 -e RABBITMQ_ERLANG_COOKIE<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rabbit&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_USER=test&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_PASS=test&#39;</span> -e CONSUL_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;10.0.75.1&#39;</span> rabbitmq:consul
</span></span><span style=display:flex><span>docker run -d --rm --name rabbit2 -h rabbit2 --network rabbit -p 30002:15672 -e RABBITMQ_ERLANG_COOKIE<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rabbit&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_USER=test&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_PASS=test&#39;</span> -e CONSUL_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;10.0.75.1&#39;</span> rabbitmq:consul
</span></span><span style=display:flex><span>docker run -d --rm --name rabbit3 -h rabbit3 --network rabbit -p 30003:15672 -e RABBITMQ_ERLANG_COOKIE<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rabbit&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_USER=test&#39;</span> -e <span style=color:#e6db74>&#39;RABBITMQ_DEFAULT_PASS=test&#39;</span> -e CONSUL_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;10.0.75.1&#39;</span> rabbitmq:consul
</span></span></code></pre></div><p>You can now visit <code>http://localhost:30001</code> (or <code>30002</code> or <code>30003</code>) and see that we have a successful cluster running. Once you&rsquo;re happy with it, you can kill it all off (as we started the containers with the <code>--rm</code> flag, Docker will delete them for us when they stop):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker stop rabbit1 rabbit2 rabbit3 consul
</span></span><span style=display:flex><span>docker network rm rabbit
</span></span></code></pre></div><h2 id=nomad-rabbit-job>Nomad Rabbit Job<a hidden class=anchor aria-hidden=true href=#nomad-rabbit-job>#</a></h2><p>Now that we know our container clusters successfully, we can create a Job definition to do the same thing in Nomad. Nomad jobs are defined in HCL, a Json-like configuration language.</p><p>The jobs require a name, which datacentre it should run in, and what kind of job type it is. In this case, our job is called <code>rabbit</code> (imaginative I know), we&rsquo;ll run it in <code>dc1</code> (the default value Nomad starts with), and we&rsquo;ll make this job be a <code>service</code>, as opposed to a <code>batch</code> or <code>system</code> job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>job <span style=color:#e6db74>&#34;rabbit&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  datacenters <span style=color:#f92672>=</span> <span style=color:#f92672>[</span><span style=color:#e6db74>&#34;dc1&#34;</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>  type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;service&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  group <span style=color:#e6db74>&#34;cluster&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># tasks ...</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>The <code>group</code> is used to hold a collection of <code>task</code>s, and when allocating a job, Nomad will make sure that all tasks belonging to an instance of a group are on the same node.</p><p>So for example, if you had a 2 node Nomad cluster, and 3 instances of a group which contained 3 tasks (e.g. API, varnish, and nginx containers), Nomad might distribute the groups like so:</p><p><img loading=lazy src=nomad-allocation.png alt="image of several nodes with groups of containers"></p><p>Within the group, we can specify the number of instances we want with the <code>count</code> property, and we also specify that for both updates and migrations, only one group can be changed at a time. This means that if you decide to upgrade the container used by the job, Nomad won&rsquo;t stop all instances at once, destroying your service&rsquo;s availability!</p><p>We also specify that we want to use the health checks (defined later on) rather than the state of the task itself to determine what is healthy, and how long the task must be healthy for before we decide it&rsquo;s actually healthy, and how long it has to achieve being healthy.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>group <span style=color:#e6db74>&#34;cluster&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  count <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  update <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    max_parallel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  migrate <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    max_parallel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    health_check <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;checks&#34;</span>
</span></span><span style=display:flex><span>    min_healthy_time <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;5s&#34;</span>
</span></span><span style=display:flex><span>    healthy_deadline <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;30s&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>The <code>task</code> is our unit of work in Nomad. In this case, we are using the <code>docker</code> driver, but it also <a href=https://www.nomadproject.io/docs/drivers/index.html>supports many other drivers</a> including <code>exec</code>, <code>rkt</code> and <code>lxc</code>. We configure which image to use, and importantly that the <code>hostname</code> is the name from Nomad!</p><p>The <code>port_map</code> tells nomad which ports of the container we want to expose, and labels them. We can then refer to the ports by their labels in other parts of the configuration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>task <span style=color:#e6db74>&#34;rabbit&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  driver <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;docker&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  config <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;pondidum/rabbitmq:consul&#34;</span>
</span></span><span style=display:flex><span>    hostname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>attr.unique.hostname<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    port_map <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      amqp <span style=color:#f92672>=</span> <span style=color:#ae81ff>5672</span>
</span></span><span style=display:flex><span>      ui <span style=color:#f92672>=</span> <span style=color:#ae81ff>15672</span>
</span></span><span style=display:flex><span>      epmd <span style=color:#f92672>=</span> <span style=color:#ae81ff>4369</span>
</span></span><span style=display:flex><span>      clustering <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>The <code>env</code> section is pretty self-explanatory; they are environment variables to pass to the container. As Consul is running on the Nomad host, we use the Nomad interpolation attribute to specify the IP of the current host, and we also set the <code>RABBITMQ_ERLANG_COOKIE</code> to a specific value. In a production environment, you should be setting this value to something unguessable, possibly using the <a href=https://www.nomadproject.io/docs/job-specification/vault.html>Vault intergration</a> in Nomad to fetch a token. We can also add other settings to pass to the container here, such as <code>RABBITMQ_DEFAULT_USER</code> and <code>RABBITMQ_DEFAULT_PASS</code>. As with the cookie generation, in a production-like environment, you&rsquo;d probably want to use the Vault integration to pull the values for these variables.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>env <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  RABBITMQ_ERLANG_COOKIE <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;rabbitmq&#34;</span>
</span></span><span style=display:flex><span>  CONSUL_HOST <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>attr.unique.network.ip-address<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>The <code>resources</code> section lets us constraints on things like CPU, Memory, IOPs, and Network. In our case, we are only specifying a set of ports to expose on the network, and that we want them to be bound to specific ports on the host:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>resources <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  network <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    port <span style=color:#e6db74>&#34;amqp&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>5672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>    port <span style=color:#e6db74>&#34;ui&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>15672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>    port <span style=color:#e6db74>&#34;epmd&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>4369</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>    port <span style=color:#e6db74>&#34;clustering&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>25672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>We could select different ports to bind the container ports to, or leave out the <code>static</code> pair entirely to have Nomad map the ports to random unused ports on the host.</p><p>Finally, the <code>service</code> block integrates with service discovery (so, Consul), and allows us to register ports and health checks for our service. In the case of our RabbitMQ cluster, we already have service discovery integration via the RabbitMQ Consul plugin, so this registration is only used for the <code>check</code> feature, which is what will also be used by the <code>migrate</code> block to see if a task is healthy:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>service <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  check <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    name     <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;alive&#34;</span>
</span></span><span style=display:flex><span>    type     <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;tcp&#34;</span>
</span></span><span style=display:flex><span>    port     <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ui&#34;</span>
</span></span><span style=display:flex><span>    interval <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;10s&#34;</span>
</span></span><span style=display:flex><span>    timeout  <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2s&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>The check is using the <code>ui</code> port defined earlier to check if the UI is alive. We could also change the health check to use the <code>amqp</code> port instead, as that might be a better indication that the actual service can do useful things. We can define multiple checks, and are not limited to TCP; <code>grpc</code>, <code>http</code>, and <code>script</code> are also supported.</p><p>The entire job definition is below, and is <a href=https://github.com/Pondidum/Nomad-RabbitMQ-Demo/blob/master/rabbit/rabbit.nomad>also available in the repository</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>job <span style=color:#e6db74>&#34;rabbit&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  datacenters <span style=color:#f92672>=</span> <span style=color:#f92672>[</span><span style=color:#e6db74>&#34;dc1&#34;</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>  type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;service&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  group <span style=color:#e6db74>&#34;cluster&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    count <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    update <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      max_parallel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    migrate <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      max_parallel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>      health_check <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;checks&#34;</span>
</span></span><span style=display:flex><span>      min_healthy_time <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;5s&#34;</span>
</span></span><span style=display:flex><span>      healthy_deadline <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;30s&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    task <span style=color:#e6db74>&#34;rabbit&#34;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      driver <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;docker&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      config <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;pondidum/rabbitmq:consul&#34;</span>
</span></span><span style=display:flex><span>        hostname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>attr.unique.hostname<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        port_map <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>          amqp <span style=color:#f92672>=</span> <span style=color:#ae81ff>5672</span>
</span></span><span style=display:flex><span>          ui <span style=color:#f92672>=</span> <span style=color:#ae81ff>15672</span>
</span></span><span style=display:flex><span>          epmd <span style=color:#f92672>=</span> <span style=color:#ae81ff>4369</span>
</span></span><span style=display:flex><span>          clustering <span style=color:#f92672>=</span> <span style=color:#ae81ff>25672</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      env <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        RABBITMQ_ERLANG_COOKIE <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;generate_a_guid_-_or_something_for_this&#34;</span>
</span></span><span style=display:flex><span>        RABBITMQ_DEFAULT_USER <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;test&#34;</span>
</span></span><span style=display:flex><span>        RABBITMQ_DEFAULT_PASS <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;test&#34;</span>
</span></span><span style=display:flex><span>        CONSUL_HOST <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>attr.unique.network.ip-address<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      resources <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        network <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>          port <span style=color:#e6db74>&#34;amqp&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>5672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>          port <span style=color:#e6db74>&#34;ui&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>15672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>          port <span style=color:#e6db74>&#34;epmd&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>4369</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>          port <span style=color:#e6db74>&#34;clustering&#34;</span> <span style=color:#f92672>{</span> static <span style=color:#f92672>=</span> <span style=color:#ae81ff>25672</span> <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      service <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;rabbitmq&#34;</span>
</span></span><span style=display:flex><span>        port <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ui&#34;</span>
</span></span><span style=display:flex><span>        check <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>          name     <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;alive&#34;</span>
</span></span><span style=display:flex><span>          type     <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;tcp&#34;</span>
</span></span><span style=display:flex><span>          interval <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;10s&#34;</span>
</span></span><span style=display:flex><span>          timeout  <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;2s&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><h2 id=running-the-job>Running The Job<a hidden class=anchor aria-hidden=true href=#running-the-job>#</a></h2><p>First, make sure your console can talk to Nomad, which we can do by using the <code>server_ip</code> file again:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NOMAD_ADDR<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://</span><span style=color:#66d9ef>$(</span>cat server_ip<span style=color:#66d9ef>)</span><span style=color:#e6db74>:4646&#34;</span>
</span></span></code></pre></div><p>Now it should be possible to run the job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nomad job run rabbit/rabbit.nomad
</span></span></code></pre></div><p>After a few moments, we can visit any of the Nomad hosts, and log in to the RabbitMQ UI (<code>http://SOME_SERVER_IP:15672</code>) and you should be greeted with a running cluster with three nodes:</p><p><img loading=lazy src=rabbitmq-cluster.png alt="rabbitmq cluster dashboard"></p><h2 id=homework>Homework<a hidden class=anchor aria-hidden=true href=#homework>#</a></h2><ul><li>Kill a container on one of the nodes and see what happens (<code>vagrant ssh n2</code> then <code>docker stop &lt;SOME_CONTAINER_ID></code>)</li><li>Create an Application which you deploy to Nomad which uses service discovery to talk to RabbitMQ</li><li>Create some more Nomad clients, and try making a bigger RabbitMQ cluster</li></ul><p>If you have any questions, feel free to comment below or <a href=https://twitter.com/pondidum>send me a tweet</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://andydote.co.uk/tags/infrastructure/>infrastructure</a></li><li><a href=https://andydote.co.uk/tags/vagrant/>vagrant</a></li><li><a href=https://andydote.co.uk/tags/nomad/>nomad</a></li><li><a href=https://andydote.co.uk/tags/consul/>consul</a></li><li><a href=https://andydote.co.uk/tags/rabbitmq/>rabbitmq</a></li></ul><nav class=paginav><a class=prev href=https://andydote.co.uk/2019/03/22/hyperv-networking/><span class=title>« Prev Page</span><br><span>Hyper-V, Docker, and Networking Drama</span></a>
<a class=next href=https://andydote.co.uk/2019/01/01/immutable-infra/><span class=title>Next Page »</span><br><span>Testing Immutable Infrastructure</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>