<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>infrastructure on Andy Dote</title><link>https://andydote.co.uk/tags/infrastructure/</link><description>Recent content in infrastructure on Andy Dote</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 22 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://andydote.co.uk/tags/infrastructure/rss.xml" rel="self" type="application/rss+xml"/><item><title>The Operator Pattern in Nomad</title><link>https://andydote.co.uk/2021/11/22/nomad-operator-pattern/</link><pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/11/22/nomad-operator-pattern/</guid><description>The Operator Pattern from Kubernetes is an excellent way of handling tasks in a cluster in an automated way, for example, provisioning applications, running backups, requesting certificates, and injecting chaos testing.
As a Nomad user, I wanted to do something similar for my clusters, so I set about seeing how it would be possible. It turns out; it is much easier than I expected! While Nomad doesn&amp;rsquo;t support the idea of Custom Resource Definitions, we can achieve an operator by utilising a regular Nomad job and the nomad HTTP API.</description></item><item><title>The Problem with CPUs and Kubernetes</title><link>https://andydote.co.uk/2021/06/02/os-cpus-and-kubernetes/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/06/02/os-cpus-and-kubernetes/</guid><description>Key Takeaway: os .cpus() returns the number of cores on a Kubernetes host, not the number of cores assigned to a pod.
Investigating excessive memory usage Recently, when I was looking through a cluster health dashboard for a Kubernetes cluster, I noticed that one of the applications deployed was using a considerable amount of RAM - way more than I thought could be reasonable. Each instance (pod) of the application used approximately 8 GB of RAM, which was definitely excessive for a reasonably simple NodeJS webserver.</description></item><item><title>Adding Observability to Vault</title><link>https://andydote.co.uk/2021/05/27/vault-observe/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/05/27/vault-observe/</guid><description>One of the things I like to do when setting up a Vault cluster is to visualise all the operations Vault is performing, which helps see usage patterns changing, whether there are lots of failed requests coming in, and what endpoints are receiving the most traffic.
While Vault has a lot of data available in Prometheus telemetry, the kind of information I am after is best taken from the Audit backend.</description></item><item><title>Observability with Infrastructure as Code</title><link>https://andydote.co.uk/2021/03/01/observability-with-infrastructure-as-code/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2021/03/01/observability-with-infrastructure-as-code/</guid><description>This article was originally published on the Pulumi blog.
When using the Pulumi Automation API to create applications which can provision infrastructure, it is very handy to be able to use observability techniques to ensure the application functions correctly and to help see where performance bottlenecks are.
One of the applications I work on creates a VPC and Bastion host and then stores the credentials into a Vault instance.</description></item><item><title>Nomad Isolated Exec</title><link>https://andydote.co.uk/2020/02/29/nomad-isolated-exec/</link><pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/02/29/nomad-isolated-exec/</guid><description>One of the many features of Nomad that I like is the ability to run things other than Docker containers. It has built-in support for Java, QEMU, and Rkt, although the latter is deprecated. Besides these inbuilt &amp;ldquo;Task Drivers&amp;rdquo; there are community maintained ones too, covering Podman, LXC, Firecraker and BSD Jails, amongst others.
The one I want to talk about today, however, is called exec. This Task Driver runs any given executable, so if you have an application which you don&amp;rsquo;t want (or can&amp;rsquo;t) put into a container, you can still schedule it with Nomad.</description></item><item><title>Consul DNS Fowarding in Alpine, revisited</title><link>https://andydote.co.uk/2019/12/30/consul-alpine-dns-revisited/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/12/30/consul-alpine-dns-revisited/</guid><description>I noticed when running an Alpine based virtual machine with Consul DNS forwarding set up, that sometimes the machine couldn&amp;rsquo;t resolve *.consul domains, but not in a consistent manner. Inspecting the logs looked like the request was being made and responded to successfully, but the result was being ignored.
After a lot of googling and frustration, I was able to track down that it&amp;rsquo;s down to a difference (or optimisation) in musl libc, which glibc doesn&amp;rsquo;t do.</description></item><item><title>Nomad Good, Kubernetes Bad</title><link>https://andydote.co.uk/2019/11/21/nomad-good-kubernetes-bad/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/11/21/nomad-good-kubernetes-bad/</guid><description>I will update this post as I learn more (both positive and negative), and is here to be linked to when people ask me why I don&amp;rsquo;t like Kubernetes, and why I would pick Nomad in most situations if I chose to use an orchestrator at all.
TLDR: I don&amp;rsquo;t like complexity, and Kubernetes has more complexity than benefits.
Operational Complexity Operating Nomad is very straight forward. There are very few moving parts, so the number of things which can go wrong is significantly reduced.</description></item><item><title>Creating a Vault instance with a TLS Consul Cluster</title><link>https://andydote.co.uk/2019/10/06/vault-consul-bootstrap/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/10/06/vault-consul-bootstrap/</guid><description>So we want to set up a Vault instance, and have it&amp;rsquo;s storage be a TLS based Consul cluster. The problem is that the Consul cluster needs Vault to create the certificates for TLS, which is quite the catch-22. Luckily for us, quite easy to solve:
Start a temporary Vault instance as an intermediate ca Launch Consul cluster, using Vault to generate certificates Destroy temporary Vault instance Start a permanent Vault instance, with Consul as the store Reprovision the Consul cluster with certificates from the new Vault instance There is a repository on Github with all the scripts used, and a few more details on some options.</description></item><item><title>Consul DNS Fowarding in Ubuntu, revisited</title><link>https://andydote.co.uk/2019/09/24/consul-ubuntu-dns-revisited/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/09/24/consul-ubuntu-dns-revisited/</guid><description>I was recently using my Hashibox for a test, and I noticed the DNS resolution didn&amp;rsquo;t seem to work. This was a bit worrying, as I have written about how to do DNS resolution with Consul forwarding in Ubuntu, and apparently something is wrong with how I do it. Interestingly, the Alpine version works fine, so it appears there is something not quite working with how I am configuring Systemd-resolved.</description></item><item><title>Canary Routing with Traefik in Nomad</title><link>https://andydote.co.uk/2019/06/23/nomad-traefik-canary/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/23/nomad-traefik-canary/</guid><description>I wanted to implement canary routing for some HTTP services deployed via Nomad the other day, but rather than having the traffic split by weighting to the containers, I wanted to direct the traffic based on a header.
My first choice of tech was to use Fabio, but it only supports routing by URL prefix, and additionally with a route weight. While I was at JustDevOps in Poland, I heard about another router/loadbalancer which worked in a similar way to Fabio: Traefik.</description></item><item><title>Configuring Consul DNS Forwarding in Alpine Linux</title><link>https://andydote.co.uk/2019/05/31/consul-dns-forwarding-alpine/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/05/31/consul-dns-forwarding-alpine/</guid><description>DEPRECATED - This has a race condition! Please see this post for an updated version which works!
Following on from the post the other day on setting up DNS forwarding to Consul with SystemD, I wanted also to show how to get Consul up and running under Alpine Linux, as it&amp;rsquo;s a little more awkward in some respects.
To start with, I am going to setup Consul as a service - I didn&amp;rsquo;t do this in the Ubuntu version, as there are plenty of useful articles about that already, but that is not the case with Alpine.</description></item><item><title>Configuring Consul DNS Forwarding in Ubuntu 16.04</title><link>https://andydote.co.uk/2019/05/29/consul-dns-forwarding/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/05/29/consul-dns-forwarding/</guid><description>DEPRECATED - This doesn&amp;rsquo;t work properly Please see this post for an updated version which works!
One of the advantages of using Consul for service discovery is that besides an HTTP API, you can also query it by DNS.
The DNS server is listening on port 8600 by default, and you can query both A records or SRV records from it. SRV records are useful as they contain additional properties (priority, weight and port), and you can get multiple records back from a single query, letting you do load balancing client side:</description></item><item><title>Running a Secure RabbitMQ Cluster in Nomad</title><link>https://andydote.co.uk/2019/04/06/nomad-rabbitmq-secure/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/04/06/nomad-rabbitmq-secure/</guid><description>Last time I wrote about running a RabbitMQ cluster in Nomad, one of the main pieces of feedback I received was about the (lack) of security of the setup, so I decided to revisit this, and write about how to launch as secure RabbitMQ node in Nomad.
The things I want to cover are:
Username and Password for the management UI Secure value for the Erlang Cookie SSL for Management and AMQP As usual, the demo repository with all the code is available if you&amp;rsquo;d rather just jump into that.</description></item><item><title>RabbitMQ clustering with Consul in Nomad</title><link>https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/</link><pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/01/28/nomad-rabbitmq-consul-cluster/</guid><description>Update If you want a secure version of this cluster, see Running a Secure RabbitMQ Cluster in Nomad.
RabbitMQ is the centre of a lot of micros service architectures, and while you can cluster it manually, it is a lot easier to use some of the auto clustering plugins, such as AWS (EC2), Consul, Etcd, or Kubernetes. As I like to use Nomad for container orchestration, I thought it would be a good idea to show how to cluster RabbitMQ when it is running in a Docker container, on an unknown host (i.</description></item><item><title>Testing Immutable Infrastructure</title><link>https://andydote.co.uk/2019/01/01/immutable-infra/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/01/01/immutable-infra/</guid><description>In my previous post, I glossed over one of the most important and useful parts of Immutable Infrastructure: Testability. There are many kinds of tests we can write for our infrastructure, but they should all be focused on the machine/service and maybe it&amp;rsquo;s nearest dependencies, not the entire system.
While this post focuses on testing a full machine (both locally in a VM, and remotely as an Amazon EC2 instance), it is also possible to do most of the same kind of tests against a Docker container.</description></item><item><title>Code-free tracing with LogStash and Jaeger</title><link>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</guid><description>I wanted to show request charts (similar to the network tab in firefox) for requests across our microservices but wanted to do so in the least invasive way possible.
We already use LogStash to collect logs from multiple hosts (via FileBeat) and forward them on to ElasticSearch, so perhaps I can do something to also output from LogStash to a tracing service.
There are a number of tracing services available (AppDash, Jaeger, Zipkin), but unfortunately LogStash doesn&amp;rsquo;t have plugins for any of them or for OpenTracing.</description></item></channel></rss>