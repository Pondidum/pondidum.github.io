<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>microservices on Andy Dote</title><link>https://andydote.co.uk/tags/microservices/</link><description>Recent content in microservices on Andy Dote</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 04 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://andydote.co.uk/tags/microservices/rss.xml" rel="self" type="application/rss+xml"/><item><title>Service Mesh with Consul Connect (and Nomad)</title><link>https://andydote.co.uk/2020/05/04/service-mesh-consul-connect/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2020/05/04/service-mesh-consul-connect/</guid><description>When it comes to implementing a new feature in an application&amp;rsquo;s ecosystem, I don&amp;rsquo;t like spending my innovation tokens unless I have to, so I try not to add new tools to my infrastructure unless I really need them.
This same approach comes when I either want, need, or have been told, to implement a Service Mesh. This means I don&amp;rsquo;t instantly setup Istio. Not because it&amp;rsquo;s bad - far from it - but because it&amp;rsquo;s extra complexity I would rather avoid, unless I need it.</description></item><item><title>Feature Toggles: Reducing Coupling</title><link>https://andydote.co.uk/2019/06/11/feature-toggles-reducing-coupling/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/11/feature-toggles-reducing-coupling/</guid><description>One of the points I make in my Feature Toggles talk is that you shouldn&amp;rsquo;t be querying a toggle&amp;rsquo;s status all over your codebase. Ideally, each toggle gets checked in as few places as possible - preferably only one place. The advantage of doing this is that very little of your codebase needs to be coupled to the toggles (either the toggle itself or the library/system for managing toggles itself).</description></item><item><title>Feature Toggles: Branch by Abstraction</title><link>https://andydote.co.uk/2019/06/03/feature-toggles-branch-by-abstraction/</link><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/06/03/feature-toggles-branch-by-abstraction/</guid><description>Recently, I was asked if I could provide an example of Branch By Abstraction when dealing with feature toggles. As this has come up a few times, I thought a blog post would be a good idea so I can refer others to it later too.
The Context As usual, this is some kind of backend (micro)service, and it will send email messages somehow. We will start with two implementations of message sending: the &amp;ldquo;current&amp;rdquo; version; which is synchronous, and a &amp;ldquo;new&amp;rdquo; version; which is async.</description></item><item><title>Testing Immutable Infrastructure</title><link>https://andydote.co.uk/2019/01/01/immutable-infra/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2019/01/01/immutable-infra/</guid><description>In my previous post, I glossed over one of the most important and useful parts of Immutable Infrastructure: Testability. There are many kinds of tests we can write for our infrastructure, but they should all be focused on the machine/service and maybe it&amp;rsquo;s nearest dependencies, not the entire system.
While this post focuses on testing a full machine (both locally in a VM, and remotely as an Amazon EC2 instance), it is also possible to do most of the same kind of tests against a Docker container.</description></item><item><title>Code-free tracing with LogStash and Jaeger</title><link>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/12/22/serilog-elk-jaeger/</guid><description>I wanted to show request charts (similar to the network tab in firefox) for requests across our microservices but wanted to do so in the least invasive way possible.
We already use LogStash to collect logs from multiple hosts (via FileBeat) and forward them on to ElasticSearch, so perhaps I can do something to also output from LogStash to a tracing service.
There are a number of tracing services available (AppDash, Jaeger, Zipkin), but unfortunately LogStash doesn&amp;rsquo;t have plugins for any of them or for OpenTracing.</description></item><item><title>Microservices or Components</title><link>https://andydote.co.uk/2018/10/28/microservices-or-components/</link><pubDate>Sun, 28 Oct 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/10/28/microservices-or-components/</guid><description>One of the reasons people list for using MicroServices is that it helps enforce separation of concerns. This is usually achieved by adding a network boundary between the services. While this is useful, it&amp;rsquo;s not without costs; namely that you&amp;rsquo;ve added a set of new failure modes: the network. We can achieve the same separation of concerns within the same codebase if we put our minds to it. In fact, this is what Simon Brown calls a Modular Monolith, and DHH calls the Majestic Monolith.</description></item><item><title>Feature Toggles with Consul</title><link>https://andydote.co.uk/2018/09/06/consul-feature-toggles/</link><pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/09/06/consul-feature-toggles/</guid><description>Feature Toggles are a great way of helping to deliver working software, although there are a few things which could go wrong. See my talk Feature Toggles: The Good, The Bad and The Ugly for some interesting stories and insights on it!
I was talking with a colleague the other day about how you could go about implementing Feature Toggles in a centralised manner into an existing system, preferably with a little overhead as possible.</description></item><item><title>Managing AppSettings in Consul</title><link>https://andydote.co.uk/2018/08/07/managing-consul-appsettings/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/08/07/managing-consul-appsettings/</guid><description>Consul is a great utility to make running your microservice architecture very simple. Amongst other things, it provides Service Discovery, Health Checks, and Configuration. In this post, we are going to be looking at Configuration; not specifically how to read from Consul, but about how we put configuration data into Consul in the first place.
The usual flow for an application using Consul for configuration is as follows:
App Starts Fetches configuration from Consul Configures itself Registers in Consul for Service Discovery Ready Step 2 is very straightforward - you query the local instance of Consul&amp;rsquo;s HTTP API, and read the response into your configuration object (If you&amp;rsquo;re using Microsoft&amp;rsquo;s Configuration libraries on dotnet core, you can use the Consul.</description></item><item><title>Locking Vault Down with Policies</title><link>https://andydote.co.uk/2018/06/23/vault-locking-it-down-with-policies/</link><pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/23/vault-locking-it-down-with-policies/</guid><description>The final part of my Vault miniseries focuses on permissioning, which is provided by Vault&amp;rsquo;s Policies.
As everything in Vault is represented as a path, the policies DSL (Domain Specific Language) just needs to apply permissions to paths to lock things down. For example, to allow all operations on the cubbyhole secret engine, we would define this policy:
path &amp;#34;cubbyhole/*&amp;#34; { capabilities = [&amp;#34;create&amp;#34;, &amp;#34;read&amp;#34;, &amp;#34;update&amp;#34;, &amp;#34;delete&amp;#34;, &amp;#34;list&amp;#34;] } Vault comes with a default policy which allows token operations (such as looking up its own token info, releasing and renewing tokens), and cubbyhole access.</description></item><item><title>Secure Communication with Vault</title><link>https://andydote.co.uk/2018/06/22/vault-secure-communication/</link><pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/22/vault-secure-communication/</guid><description>I think Vault by Hashicorp is a great product - I particularly love how you can do dynamic secret generation (e.g for database connections). But how do you validate that the application requesting the secret is allowed to perform that action? How do you know it&amp;rsquo;s not someone or something impersonating your application?
While musing this at an airport the other day, my colleague Patrik sent me a link to a StackOverflow post about this very question</description></item><item><title>Managing Postgres Connection Strings with Vault</title><link>https://andydote.co.uk/2018/06/17/secret-management-vault-postgres-connection/</link><pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2018/06/17/secret-management-vault-postgres-connection/</guid><description>One of the points I made in my recent NDC talk on 12 Factor microservices, was that you shouldn&amp;rsquo;t be storing sensitive data, such as API keys, usernames, passwords etc. in the environment variables.
Don&amp;rsquo;t Store Sensitive Data in the Environment
My reasoning is that when you were accessing Environment Variables in Heroku&amp;rsquo;s platform, you were actually accessing some (probably) secure key-value store, rather than actual environment variables.
While you can use something like Consul&amp;rsquo;s key-value store for this, it&amp;rsquo;s not much better as it still stores all the values in plaintext, and has no auditing or logging.</description></item><item><title>Preventing MicroService Boilerplate</title><link>https://andydote.co.uk/2016/07/17/preventing-microservice-boilerplate/</link><pubDate>Sun, 17 Jul 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/07/17/preventing-microservice-boilerplate/</guid><description>One of the downsides to microservices I have found is that I end up repeating the same blocks of code over and over for each service. Not only that, but the project setup is repetitive, as all the services use the Single Project Service and Console method.
What do we do in every service? Initialise Serilog. Add a Serilog sink to ElasticSearch for Kibana (but only in non-local config.) Hook/Unhook the AppDomain.</description></item><item><title>Database Integrations for MicroServices</title><link>https://andydote.co.uk/2016/06/09/database-integrations-for-microservices/</link><pubDate>Thu, 09 Jun 2016 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2016/06/09/database-integrations-for-microservices/</guid><description>This is a follow up post after seeing Michal Franc&amp;rsquo;s NDC talk on migrating from Monolithic architectures.
One point raised was that Database Integration points are a terrible idea - and I wholeheartedly agree. However, there can be a number of situations where a Database Integration is the best or only way to achieve the end goal. This can be either technical; say a tool does not support API querying (looking at you SSRS), or cultural; the other team either don&amp;rsquo;t have the willingness, time, or power to learn how to query an API.</description></item><item><title>Running pre-compiled microservices in Docker with Mono</title><link>https://andydote.co.uk/2015/09/15/pre-compiled-microservices/</link><pubDate>Tue, 15 Sep 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/09/15/pre-compiled-microservices/</guid><description>Last time we went through creating a Dockerfile for a microservice, with the service being compiled on creation of the container image, using xbuild.
However we might not want to compile the application to create the container image, and use an existing version (e.g. one created by a build server.)
Our original Dockerfile was this:
FROM mono:3.10-onbuild RUN apt-get update &amp;amp;&amp;amp; apt-get install mono-4.0-service -y CMD [ &amp;#34;mono-service&amp;#34;, &amp;#34;./MicroServiceDemo.exe&amp;#34;, &amp;#34;--no-daemon&amp;#34; ] EXPOSE 12345 We only need to make a few modifications to use a pre-compiled application:</description></item><item><title>Running microservices in Docker with Mono</title><link>https://andydote.co.uk/2015/09/05/running-microservices-in-docker-with-mono/</link><pubDate>Sat, 05 Sep 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/09/05/running-microservices-in-docker-with-mono/</guid><description>Getting a service running under Docker is fairly straight forward once you have all the working parts together. I have an app written (following my guide on service and console in one), which uses Owin to serve a web page as a demo:
install-package Microsoft.Owin.SelfHost public partial class Service : ServiceBase { //see the service console post for the rest of this protected override void OnStart(string[] args) { _app = WebApp.</description></item><item><title>A single project Windows Service and Console</title><link>https://andydote.co.uk/2015/08/30/single-project-service-and-console/</link><pubDate>Sun, 30 Aug 2015 00:00:00 +0000</pubDate><guid>https://andydote.co.uk/2015/08/30/single-project-service-and-console/</guid><description>I have found that when developing MicroServices, I often want to run them from within Visual Studio, or just as a console application, and not have to bother with the hassle of installing as windows services.
In the past I have seen this achieved by creating a Class Library project with all the actual implementation inside it, and then both a Console Application and Windows Service project referencing the library and doing nothing other than calling a .</description></item></channel></rss>